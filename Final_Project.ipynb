{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Run this to download the dataset if not using kaggle**","metadata":{}},{"cell_type":"code","source":"import os\n\n# CHANGE THIS to where you want the dataset\nCOCO_ROOT = \"/kaggle/input/coco-2017-dataset\"  \n\nIMAGES_DIR = COCO_ROOT  # train2017/ and val2017/ will live directly under this\nANN_DIR = os.path.join(COCO_ROOT, \"annotations\")\n\nos.makedirs(COCO_ROOT, exist_ok=True)\nos.makedirs(ANN_DIR, exist_ok=True)\n\nCOCO_ROOT, IMAGES_DIR, ANN_DIR\n\n# This cell uses IPython's ! to run shell commands.\n# It will:\n#  - download train2017.zip\n#  - download val2017.zip\n#  - download annotations_trainval2017.zip\n\nprint(\"Downloading MS COCO 2017 train/val + annotations to\", COCO_ROOT)\n\n# Train images\n!cd \"$COCO_ROOT\" && wget -c http://images.cocodataset.org/zips/train2017.zip\n\n# Val images\n!cd \"$COCO_ROOT\" && wget -c http://images.cocodataset.org/zips/val2017.zip\n\n# Train/Val annotations (includes captions)\n!cd \"$COCO_ROOT\" && wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n\n\n# Unzip train and val images into COCO_ROOT\n!cd \"$COCO_ROOT\" && unzip -q train2017.zip\n!cd \"$COCO_ROOT\" && unzip -q val2017.zip\n\n# Unzip annotations into COCO_ROOT/annotations\n!cd \"$COCO_ROOT\" && unzip -q annotations_trainval2017.zip -d \"$ANN_DIR\"\n\n\nimport glob\nfor z in glob.glob(os.path.join(COCO_ROOT, \"*.zip\")):\n    print(\"Removing\", z)\n    os.remove(z)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# === Cell 1: Install dependencies and set global config ===\n","metadata":{}},{"cell_type":"code","source":"\n!pip install timm open_clip_torch nltk pycocotools -q\n\nimport os, json, re, random\nfrom collections import Counter, defaultdict\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nfrom torchvision import transforms\nimport timm\nimport open_clip\nimport nltk\nnltk.download('punkt', quiet=True)\n\nprint(\"Torch:\", torch.__version__)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ==== PATHS & CONFIG (adjust as needed) ====\n# COCO 2017 dataset root on Kaggle (change if different)\nCOCO_ROOT    = \"/kaggle/input/coco-2017-dataset/coco2017\"\nPROCESSED_DIR = \"/kaggle/working/processed\"\nos.makedirs(PROCESSED_DIR, exist_ok=True)\n\nVOCAB_PATH   = os.path.join(PROCESSED_DIR, \"vocab.json\")\n\nMAX_LEN      = 30          # max caption length (incl <bos>/<eos>)\nBATCH_SIZE   = 64\nFREQ_THRESH  = 4           # min word frequency to keep in vocab\nDEBUG_LIMIT  = 100000       # set to None for full train; smaller for faster dev\nVAL_DEBUG    = 5000        # small val subset for faster eval\n\n# Beam search config\nBEAM_SIZE    = 3           # you can increase to 5 for better quality (but slower)\nNO_REPEAT_NGRAM_SIZE = 3   # e.g., 3 to avoid repeating same 3-gram (0 disables)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:59:25.685004Z","iopub.execute_input":"2025-12-09T07:59:25.685267Z","iopub.status.idle":"2025-12-09T07:59:41.240566Z","shell.execute_reply.started":"2025-12-09T07:59:25.685245Z","shell.execute_reply":"2025-12-09T07:59:41.239724Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Torch: 2.6.0+cu124\nUsing device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# === Cell 2: Vocabulary, tokenization, COCO dataset, collate ===\n","metadata":{}},{"cell_type":"code","source":"\nSPECIAL_TOKENS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n\nclass Vocabulary:\n    def __init__(self, freq_threshold: int = 5):\n        self.freq_threshold = freq_threshold\n        self.itos: List[str] = []\n        self.stoi: Dict[str, int] = {}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def build_vocab(self, counter: Counter):\n        self.itos = SPECIAL_TOKENS.copy()\n        for word, freq in counter.items():\n            if freq >= self.freq_threshold:\n                self.itos.append(word)\n        self.stoi = {w: i for i, w in enumerate(self.itos)}\n        print(f\"Vocab built: {len(self.itos)} tokens (freq ≥ {self.freq_threshold})\")\n\n    def numericalize(self, tokens: List[str]) -> List[int]:\n        return [self.stoi.get(tok, self.stoi[\"<unk>\"]) for tok in tokens]\n\n    def save(self, path: str):\n        obj = {\"itos\": self.itos, \"freq_threshold\": self.freq_threshold}\n        with open(path, \"w\") as f:\n            json.dump(obj, f)\n        print(f\"Saved vocab to {path}\")\n\n    @classmethod\n    def load(cls, path: str) -> \"Vocabulary\":\n        with open(path, \"r\") as f:\n            obj = json.load(f)\n        vocab = cls(freq_threshold=obj.get(\"freq_threshold\", 5))\n        vocab.itos = obj[\"itos\"]\n        vocab.stoi = {w: i for i, w in enumerate(vocab.itos)}\n        print(f\"Loaded vocab from {path}, size={len(vocab)}\")\n        return vocab\n\ndef tokenize_caption(text: str) -> List[str]:\n    text = text.lower().strip()\n    text = re.sub(r\"[.?!]+$\", \"\", text)\n    return text.split()\n\nclass COCODataset(Dataset):\n    def __init__(\n        self,\n        images_root: str,\n        captions_json: str,\n        vocab: Vocabulary,\n        max_len: int = 30,\n        transform=None,\n        debug_limit: int = None,\n    ):\n        self.images_root = images_root\n        self.vocab = vocab\n        self.max_len = max_len\n        self.transform = transform or self._default_transform()\n\n        with open(captions_json, \"r\") as f:\n            ann = json.load(f)\n\n        self.imgs = {img[\"id\"]: img for img in ann[\"images\"]}\n\n        self.samples = []\n        for a in ann[\"annotations\"]:\n            img_id = a[\"image_id\"]\n            caption = a[\"caption\"]\n            tokens = tokenize_caption(caption)\n            self.samples.append((self.imgs[img_id][\"file_name\"], tokens))\n\n        if debug_limit is not None:\n            self.samples = self.samples[:debug_limit]\n            print(f\"[COCODataset] Debug limit: {len(self.samples)} samples\")\n\n        print(f\"Loaded {len(self.samples)} (image, caption) pairs from {captions_json}\")\n\n    def _default_transform(self):\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n        ])\n\n    def __len__(self) -> int:\n        return len(self.samples)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        file_name, tokens = self.samples[idx]\n        img_path = os.path.join(self.images_root, file_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n\n        bos_id = self.vocab.stoi[\"<bos>\"]\n        eos_id = self.vocab.stoi[\"<eos>\"]\n        pad_id = self.vocab.stoi[\"<pad>\"]\n\n        seq_ids = [bos_id] + self.vocab.numericalize(tokens) + [eos_id]\n        if len(seq_ids) < self.max_len:\n            seq_ids += [pad_id] * (self.max_len - len(seq_ids))\n        else:\n            seq_ids = seq_ids[:self.max_len]\n\n        caption = torch.tensor(seq_ids, dtype=torch.long)\n\n        return {\n            \"image\": image,\n            \"caption\": caption,\n            \"file_name\": file_name,\n        }\n\ndef coco_collate_fn(batch):\n    images = torch.stack([b[\"image\"] for b in batch], dim=0)\n    captions = torch.stack([b[\"caption\"] for b in batch], dim=0)\n    file_names = [b[\"file_name\"] for b in batch]\n    return {\n        \"image\": images,\n        \"caption\": captions,\n        \"file_name\": file_names,\n    }\n\ndef simple_tokenize(text: str):\n    return re.findall(r\"[a-z]+\", text.lower())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:59:46.023348Z","iopub.execute_input":"2025-12-09T07:59:46.023864Z","iopub.status.idle":"2025-12-09T07:59:46.040631Z","shell.execute_reply.started":"2025-12-09T07:59:46.023837Z","shell.execute_reply":"2025-12-09T07:59:46.039892Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# === Cell 3: Build or load vocabulary from COCO train captions \n","metadata":{}},{"cell_type":"code","source":"\nTRAIN_CAPTIONS_JSON = os.path.join(COCO_ROOT, \"annotations\", \"captions_train2017.json\")\n\nif os.path.exists(VOCAB_PATH):\n    vocab = Vocabulary.load(VOCAB_PATH)\nelse:\n    with open(TRAIN_CAPTIONS_JSON, \"r\") as f:\n        train_ann = json.load(f)\n    print(\"Num captions:\", len(train_ann[\"annotations\"]))\n\n    counter = Counter()\n    for ann in tqdm(train_ann[\"annotations\"], desc=\"Counting words for vocab\"):\n        tokens = tokenize_caption(ann[\"caption\"])\n        counter.update(tokens)\n\n    vocab = Vocabulary(freq_threshold=FREQ_THRESH)\n    vocab.build_vocab(counter)\n    vocab.save(VOCAB_PATH)\n\npad_id = vocab.stoi[\"<pad>\"]\nbos_id = vocab.stoi[\"<bos>\"]\neos_id = vocab.stoi[\"<eos>\"]\nprint(\"pad/bos/eos ids:\", pad_id, bos_id, eos_id)\nprint(\"Vocab size:\", len(vocab))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:59:51.468475Z","iopub.execute_input":"2025-12-09T07:59:51.469252Z","iopub.status.idle":"2025-12-09T07:59:55.827893Z","shell.execute_reply.started":"2025-12-09T07:59:51.469223Z","shell.execute_reply":"2025-12-09T07:59:55.827125Z"}},"outputs":[{"name":"stdout","text":"Num captions: 591753\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Counting words for vocab:   0%|          | 0/591753 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"283cd214bec94192964404f1f54afc1a"}},"metadata":{}},{"name":"stdout","text":"Vocab built: 12822 tokens (freq ≥ 4)\nSaved vocab to /kaggle/working/processed/vocab.json\npad/bos/eos ids: 0 1 2\nVocab size: 12822\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Cell 4: Vision encoder (ViT), Transformer decoder, Captioner \n","metadata":{}},{"cell_type":"code","source":"\nclass ViTEncoder(nn.Module):\n    def __init__(\n        self,\n        model_name: str = \"vit_base_patch16_224\",\n        pretrained: bool = True,\n        trainable: bool = False,\n        d_model: int = 512,\n    ):\n        super().__init__()\n        self.vit = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n        )\n        self.vit.reset_classifier(0)\n        vit_dim = self.vit.num_features\n        if vit_dim != d_model:\n            self.proj = nn.Linear(vit_dim, d_model)\n        else:\n            self.proj = nn.Identity()\n\n        for p in self.vit.parameters():\n            p.requires_grad = trainable\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        feats = self.vit.forward_features(x)  # [B, S, C] or [B, C] depending on timm version\n        if feats.dim() == 2:\n            feats = feats.unsqueeze(1)        # [B, 1, C]\n        feats = self.proj(feats)             # [B, S, d_model]\n        return feats\n\nclass TransformerCaptionDecoder(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        d_model: int = 512,\n        num_layers: int = 6,\n        num_heads: int = 8,\n        dim_feedforward: int = 2048,\n        max_len: int = 30,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.max_len = max_len\n\n        self.token_embed = nn.Embedding(vocab_size, d_model)\n        self.pos_embed = nn.Embedding(max_len, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=d_model,\n            nhead=num_heads,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        self.out_proj = nn.Linear(d_model, vocab_size)\n\n    def forward(self, tgt, memory, tgt_key_padding_mask=None):\n        B, T = tgt.shape\n        positions = torch.arange(0, T, device=tgt.device).unsqueeze(0).expand(B, T)\n        x = self.token_embed(tgt) * (self.d_model ** 0.5)\n        x = x + self.pos_embed(positions)\n        x = self.dropout(x)\n\n        causal_mask = torch.triu(\n            torch.ones(T, T, device=tgt.device, dtype=torch.bool),\n            diagonal=1,\n        )\n\n        x = self.decoder(\n            tgt=x,\n            memory=memory,\n            tgt_mask=causal_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask,\n        )\n        logits = self.out_proj(x)\n        return logits\n\nclass Captioner(nn.Module):\n    def __init__(self, vocab_size: int, max_len: int = 30, d_model: int = 512, vit_trainable: bool = False):\n        super().__init__()\n        self.encoder = ViTEncoder(d_model=d_model, trainable=vit_trainable)\n        self.decoder = TransformerCaptionDecoder(\n            vocab_size=vocab_size,\n            d_model=d_model,\n            max_len=max_len,\n        )\n        self.max_len = max_len\n\n    def forward(self, images: torch.Tensor, captions_in: torch.Tensor) -> torch.Tensor:\n        memory = self.encoder(images)\n        logits = self.decoder(captions_in, memory)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:59:57.353150Z","iopub.execute_input":"2025-12-09T07:59:57.353429Z","iopub.status.idle":"2025-12-09T07:59:57.364336Z","shell.execute_reply.started":"2025-12-09T07:59:57.353408Z","shell.execute_reply":"2025-12-09T07:59:57.363578Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# === Cell 5: Greedy decoding and Beam search decoding ===\n","metadata":{}},{"cell_type":"code","source":"\n@torch.no_grad()\ndef greedy_decode_batch(model, images, bos_id, eos_id, max_len, device):\n    model.eval()\n    B = images.size(0)\n    images = images.to(device)\n    with torch.no_grad():\n        memory = model.encoder(images)\n        ys = torch.full((B, 1), bos_id, device=device, dtype=torch.long)\n        finished = torch.zeros(B, dtype=torch.bool, device=device)\n\n        for _ in range(max_len - 1):\n            logits = model.decoder(ys, memory)  # [B, t, V]\n            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n            ys = torch.cat([ys, next_token], dim=1)\n            finished |= (next_token.squeeze(-1) == eos_id)\n            if finished.all():\n                break\n    return ys  # [B, T]\n\n@torch.no_grad()\ndef beam_search_decode_batch(\n    model,\n    images,\n    bos_id,\n    eos_id,\n    max_len,\n    device,\n    beam_size=3,\n    no_repeat_ngram_size=0,\n):\n    \"\"\"\n    Beam search for each image in the batch independently.\n    Returns tensor [B, T] of best sequences.\n    \"\"\"\n    model.eval()\n    images = images.to(device)\n    B = images.size(0)\n\n    with torch.no_grad():\n        memory = model.encoder(images)   # [B, S, D]\n\n    sequences = []\n    for i in range(B):\n        mem_i = memory[i:i+1]           # [1, S, D]\n        seq = beam_search_single(\n            model,\n            mem_i,\n            bos_id,\n            eos_id,\n            max_len,\n            device,\n            beam_size,\n            no_repeat_ngram_size,\n        )\n        sequences.append(seq)\n\n    max_T = max(len(s) for s in sequences)\n    out = torch.full((B, max_T), eos_id, device=device, dtype=torch.long)\n    for i, seq in enumerate(sequences):\n        out[i, :len(seq)] = torch.tensor(seq, device=device, dtype=torch.long)\n    return out\n\ndef _has_repeat_ngram(candidate: List[int], no_repeat_ngram_size: int) -> bool:\n    if no_repeat_ngram_size <= 0:\n        return False\n    if len(candidate) < 2 * no_repeat_ngram_size:\n        return False\n    seen = set()\n    for i in range(len(candidate) - no_repeat_ngram_size + 1):\n        ngram = tuple(candidate[i:i+no_repeat_ngram_size])\n        if ngram in seen:\n            return True\n        seen.add(ngram)\n    return False\n\n@torch.no_grad()\ndef beam_search_single(\n    model,\n    memory,                # [1, S, D] for a single image\n    bos_id,\n    eos_id,\n    max_len,\n    device,\n    beam_size=3,\n    no_repeat_ngram_size=0,\n):\n    \"\"\"\n    Beam search for a single image.\n    Returns a list of token ids (best sequence).\n    \"\"\"\n    V = model.decoder.vocab_size\n\n    beam = [(0.0, [bos_id])]\n    completed = []\n\n    for _ in range(max_len - 1):\n        new_beam = []\n        for logp, seq in beam:\n            if seq[-1] == eos_id:\n                completed.append((logp, seq))\n                continue\n\n            tgt = torch.tensor(seq, device=device, dtype=torch.long).unsqueeze(0)  # [1, t]\n            logits = model.decoder(tgt, memory)  # [1, t, V]\n            next_logits = logits[:, -1, :]       # [1, V]\n            probs = torch.log_softmax(next_logits, dim=-1).squeeze(0)  # [V]\n\n            topk_logp, topk_ids = probs.topk(beam_size)\n\n            for lp, idx in zip(topk_logp.tolist(), topk_ids.tolist()):\n                candidate = seq + [idx]\n                if no_repeat_ngram_size > 0 and _has_repeat_ngram(candidate, no_repeat_ngram_size):\n                    continue\n                new_beam.append((logp + lp, candidate))\n\n        if not new_beam:\n            break\n\n        new_beam.sort(key=lambda x: x[0], reverse=True)\n        beam = new_beam[:beam_size]\n\n        if len(completed) >= beam_size:\n            break\n\n    if completed:\n        completed.sort(key=lambda x: x[0], reverse=True)\n        best_seq = completed[0][1]\n    else:\n        beam.sort(key=lambda x: x[0], reverse=True)\n        best_seq = beam[0][1]\n\n    return best_seq\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T08:00:05.654777Z","iopub.execute_input":"2025-12-09T08:00:05.655474Z","iopub.status.idle":"2025-12-09T08:00:05.668325Z","shell.execute_reply.started":"2025-12-09T08:00:05.655449Z","shell.execute_reply":"2025-12-09T08:00:05.667553Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# === Cell 6: CLIP grounding loss and ID->text helpers ===\n","metadata":{}},{"cell_type":"code","source":"\nclass CLIPGroundingLoss(nn.Module):\n    def __init__(\n        self,\n        clip_model,\n        clip_tokenizer,\n        device,\n        lambda_clip: float = 0.5,\n        input_mean=(0.485, 0.456, 0.406),\n        input_std=(0.229, 0.224, 0.225),\n    ):\n        super().__init__()\n        self.clip_model = clip_model.eval()\n        self.clip_tokenizer = clip_tokenizer\n        self.device = device\n        self.lambda_clip = lambda_clip\n\n        self.input_mean = torch.tensor(input_mean).view(1, 3, 1, 1).to(device)\n        self.input_std  = torch.tensor(input_std).view(1, 3, 1, 1).to(device)\n\n        self.clip_mean = torch.tensor(\n            [0.48145466, 0.4578275, 0.40821073]\n        ).view(1, 3, 1, 1).to(device)\n        self.clip_std = torch.tensor(\n            [0.26862954, 0.26130258, 0.27577711]\n        ).view(1, 3, 1, 1).to(device)\n\n        for p in self.clip_model.parameters():\n            p.requires_grad = False\n\n    @torch.no_grad()\n    def _encode_image(self, images: torch.Tensor) -> torch.Tensor:\n        imgs = images * self.input_std + self.input_mean\n        imgs = imgs.clamp(0, 1)\n        imgs = (imgs - self.clip_mean) / self.clip_std\n        img_feats = self.clip_model.encode_image(imgs)\n        img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n        return img_feats\n\n    @torch.no_grad()\n    def _encode_text(self, captions: List[str]) -> torch.Tensor:\n        tokens = self.clip_tokenizer(captions).to(self.device)\n        txt_feats = self.clip_model.encode_text(tokens)\n        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n        return txt_feats\n\n    def forward(self, images: torch.Tensor, captions_text: List[str]) -> torch.Tensor:\n        img_feats = self._encode_image(images)\n        txt_feats = self._encode_text(captions_text)\n        sim = (img_feats * txt_feats).sum(dim=-1)\n        loss = (1.0 - sim).mean()\n        return self.lambda_clip * loss\n\ndef ids_to_words(ids, vocab, pad_id, eos_id, drop_bos=True):\n    words = []\n    for i in ids:\n        if i == pad_id:\n            break\n        w = vocab.itos[i]\n        if drop_bos and w == \"<bos>\":\n            continue\n        if w == \"<eos>\":\n            break\n        words.append(w)\n    return words\n\ndef batch_ids_to_strings(batch_ids, vocab, pad_id, eos_id):\n    if isinstance(batch_ids, torch.Tensor):\n        batch_ids = batch_ids.tolist()\n    sentences = []\n    for ids in batch_ids:\n        words = ids_to_words(ids, vocab, pad_id, eos_id)\n        sentences.append(\" \".join(words))\n    return sentences\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T08:00:10.371951Z","iopub.execute_input":"2025-12-09T08:00:10.372239Z","iopub.status.idle":"2025-12-09T08:00:10.382189Z","shell.execute_reply.started":"2025-12-09T08:00:10.372216Z","shell.execute_reply":"2025-12-09T08:00:10.381499Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Cell 7: Build train/val datasets, dataloaders, model, optimizer, CLIP\n","metadata":{}},{"cell_type":"code","source":"\nTRAIN_IMAGES_DIR = os.path.join(COCO_ROOT, \"train2017\")\nVAL_IMAGES_DIR   = os.path.join(COCO_ROOT, \"val2017\")\nANN_DIR          = os.path.join(COCO_ROOT, \"annotations\")\nTRAIN_JSON       = os.path.join(ANN_DIR, \"captions_train2017.json\")\nVAL_JSON         = os.path.join(ANN_DIR, \"captions_val2017.json\")\n\ntrain_dataset = COCODataset(\n    images_root=TRAIN_IMAGES_DIR,\n    captions_json=TRAIN_JSON,\n    vocab=vocab,\n    max_len=MAX_LEN,\n    debug_limit=DEBUG_LIMIT,\n)\n\nval_dataset = COCODataset(\n    images_root=VAL_IMAGES_DIR,\n    captions_json=VAL_JSON,\n    vocab=vocab,\n    max_len=MAX_LEN,\n    debug_limit=VAL_DEBUG,\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,          # keep 0 to avoid multiprocessing issues\n    collate_fn=coco_collate_fn,\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=coco_collate_fn,\n)\n\nmodel = Captioner(\n    vocab_size=len(vocab),\n    max_len=MAX_LEN,\n    d_model=512,\n    vit_trainable=True,    # start frozen; you can set True later to fine-tune ViT\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_id)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n# CLIP model for grounding\nclip_model, _, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\nclip_model = clip_model.to(device)\nclip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\nclip_loss_fn = CLIPGroundingLoss(clip_model, clip_tokenizer, device, lambda_clip=0.0)\n\nprint(\"Setup done. Train samples:\", len(train_dataset), \"Val samples:\", len(val_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T08:00:14.530613Z","iopub.execute_input":"2025-12-09T08:00:14.530919Z"}},"outputs":[{"name":"stdout","text":"[COCODataset] Debug limit: 10000 samples\nLoaded 10000 (image, caption) pairs from /kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json\n[COCODataset] Debug limit: 1000 samples\nLoaded 1000 (image, caption) pairs from /kaggle/input/coco-2017-dataset/coco2017/annotations/captions_val2017.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52bee78e17e47c78554ec3c3c0e899d"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# === Cell 8: Training loop with optional CLIP grounding (CE + lambda * CLIP) ===\n","metadata":{}},{"cell_type":"code","source":"\ndef train_one_epoch_with_clip(\n    model,\n    loader,\n    optimizer,\n    criterion,\n    device,\n    vocab,\n    clip_loss_fn: CLIPGroundingLoss = None,\n    lambda_clip_current: float = 0.0,\n    max_len: int = 30,\n):\n    model.train()\n    total_loss = total_ce = total_clip = 0.0\n\n    bos_id = vocab.stoi[\"<bos>\"]\n    eos_id = vocab.stoi[\"<eos>\"]\n    pad_id = vocab.stoi[\"<pad>\"]\n\n    for batch in tqdm(loader, desc=\"Train\"):\n        images = batch[\"image\"].to(device)\n        captions = batch[\"caption\"].to(device)\n\n        inputs = captions[:, :-1]\n        targets = captions[:, 1:]\n\n        logits = model(images, inputs)\n        B, T, V = logits.shape\n        ce_loss = criterion(\n            logits.reshape(B * T, V),\n            targets.reshape(B * T),\n        )\n\n        if clip_loss_fn is not None and lambda_clip_current > 0:\n            with torch.no_grad():\n                pred_ids_batch = greedy_decode_batch(\n                    model, images, bos_id, eos_id, max_len, device\n                )\n            captions_text = batch_ids_to_strings(pred_ids_batch, vocab, pad_id, eos_id)\n            clip_loss_fn.lambda_clip = lambda_clip_current\n            clip_loss = clip_loss_fn(images, captions_text)\n        else:\n            clip_loss = torch.tensor(0.0, device=device)\n\n        loss = ce_loss + clip_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_ce += ce_loss.item()\n        total_clip += clip_loss.item()\n\n    n = len(loader)\n    return total_loss / n, total_ce / n, total_clip / n\n\n# === Run training ===\nEPOCHS = 5\nLAMBDA_CLIP_TARGET = 0.0   # adjust; try 0.3–0.5\nWARMUP_EPOCHS = 2          # ramp CLIP from 0 to target over first few epochs\n\nfor epoch in range(EPOCHS):\n    if LAMBDA_CLIP_TARGET > 0:\n        if epoch < WARMUP_EPOCHS:\n            lambda_clip_current = LAMBDA_CLIP_TARGET * (epoch + 1) / WARMUP_EPOCHS\n        else:\n            lambda_clip_current = LAMBDA_CLIP_TARGET\n    else:\n        lambda_clip_current = 0.0\n\n    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} | lambda_clip={lambda_clip_current:.3f} ===\")\n    avg_loss, avg_ce, avg_clip = train_one_epoch_with_clip(\n        model, train_loader, optimizer, criterion,\n        device, vocab, clip_loss_fn, lambda_clip_current, MAX_LEN\n    )\n    print(f\"Train: total={avg_loss:.4f} | CE={avg_ce:.4f} | CLIP={avg_clip:.4f}\")\n\n    torch.save(model.state_dict(), f\"/kaggle/working/model_epoch{epoch+1}.pt\")\n    print(\"Saved checkpoint for epoch\", epoch+1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# === Cell 9: Evaluation – BLEU (beam search) and CLIPScore (beam search) ===\n","metadata":{}},{"cell_type":"code","source":"\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef evaluate_bleu_on_val_beam(model, val_loader, vocab, device, max_batches=30):\n    model.eval()\n    smoothie = SmoothingFunction().method4\n    scores = []\n\n    for b_idx, batch in enumerate(tqdm(val_loader, desc=\"BLEU eval (beam)\")):\n        if b_idx >= max_batches:\n            break\n        images = batch[\"image\"]\n        captions = batch[\"caption\"]\n\n        pred_ids_batch = beam_search_decode_batch(\n            model, images, bos_id, eos_id, MAX_LEN, device,\n            beam_size=BEAM_SIZE,\n            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n        )\n\n        B = captions.size(0)\n        for i in range(B):\n            gt_ids   = captions[i].tolist()\n            pred_ids = pred_ids_batch[i].tolist()\n\n            gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n            pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n            if len(pred_words) == 0 or len(gt_words) == 0:\n                continue\n\n            score = sentence_bleu(\n                [gt_words],\n                pred_words,\n                smoothing_function=smoothie,\n                weights=(0.25, 0.25, 0.25, 0.25),\n            )\n            scores.append(score)\n\n    return float(np.mean(scores)) if scores else 0.0\n\ndef compute_clipscore_on_val_beam(model, val_loader, clip_model, clip_tokenizer, vocab, device, max_batches=30):\n    model.eval()\n    clip_model.eval()\n    sims = []\n\n    for b_idx, batch in enumerate(tqdm(val_loader, desc=\"CLIPScore eval (beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"].to(device)\n\n        pred_ids_batch = beam_search_decode_batch(\n            model, images, bos_id, eos_id, MAX_LEN, device,\n            beam_size=BEAM_SIZE,\n            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n        )\n        captions_text = batch_ids_to_strings(pred_ids_batch, vocab, pad_id, eos_id)\n\n        with torch.no_grad():\n            # Using open_clip's preprocess pipeline\n            # Recreate transforms from create_model_and_transforms:\n            # we didn't save the preprocess, so just recreate:\n            _, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n            for img, cap in zip(batch[\"image\"], captions_text):\n                image = transforms.ToPILImage()(img.cpu())\n                image = preprocess(image).unsqueeze(0).to(device)\n                tokens = clip_tokenizer([cap]).to(device)\n\n                img_feat = clip_model.encode_image(image)\n                txt_feat = clip_model.encode_text(tokens)\n\n                img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n                txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n                sim = (img_feat * txt_feat).sum(dim=-1).item()\n                sims.append(sim)\n\n    sims = np.array(sims)\n    return {\n        \"mean\": float(sims.mean()),\n        \"median\": float(np.median(sims)),\n        \"std\": float(sims.std()),\n        \"low_clip_rate\": float((sims < 0.2).mean()),\n    }\n\nbleu_val = evaluate_bleu_on_val_beam(model, val_loader, vocab, device, max_batches=32)\nclip_stats = compute_clipscore_on_val_beam(model, val_loader, clip_model, clip_tokenizer, vocab, device, max_batches=32)\n\nprint(\"BLEU-4 (beam) approx:\", bleu_val)\nprint(\"CLIPScore stats (beam):\", clip_stats)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# === Cell 10: CHAIR hallucination metrics (beam captions) ===\n","metadata":{}},{"cell_type":"code","source":"\nINST_VAL_JSON = os.path.join(ANN_DIR, \"instances_val2017.json\")\nwith open(INST_VAL_JSON, \"r\") as f:\n    inst_val = json.load(f)\n\nimgid_to_file = {img[\"id\"]: img[\"file_name\"] for img in inst_val[\"images\"]}\ncatid_to_name = {c[\"id\"]: c[\"name\"].lower() for c in inst_val[\"categories\"]}\n\nfile_to_objects = defaultdict(set)\nfor ann in inst_val[\"annotations\"]:\n    img_id = ann[\"image_id\"]\n    cat_id = ann[\"category_id\"]\n    file_name = imgid_to_file[img_id]\n    cat_name = catid_to_name[cat_id]\n    file_to_objects[file_name].add(cat_name)\n\ncategory_vocab = []\nfor cat_name in sorted(set(catid_to_name.values())):\n    tokens = simple_tokenize(cat_name)\n    category_vocab.append((cat_name, tokens))\n\ndef find_mentioned_objects(caption_text: str, category_vocab):\n    tokens = simple_tokenize(caption_text)\n    mentioned = set()\n    for cat_name, cat_tokens in category_vocab:\n        L = len(cat_tokens)\n        if L == 1:\n            if cat_tokens[0] in tokens:\n                mentioned.add(cat_name)\n        else:\n            for i in range(len(tokens) - L + 1):\n                if tokens[i:i+L] == cat_tokens:\n                    mentioned.add(cat_name)\n                    break\n    return mentioned\n\ndef compute_chair_on_val_beam(\n    model,\n    val_loader,\n    file_to_objects,\n    category_vocab,\n    vocab,\n    device,\n    max_batches: int = 30,\n):\n    model.eval()\n    all_caps = 0\n    caps_with_hallucination = 0\n    total_mentions = 0\n    hallucinated_mentions = 0\n\n    for b_idx, batch in enumerate(tqdm(val_loader, desc=\"CHAIR eval (beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        file_names = batch[\"file_name\"]\n        all_caps += len(file_names)\n\n        pred_ids_batch = beam_search_decode_batch(\n            model, images, bos_id, eos_id, MAX_LEN, device,\n            beam_size=BEAM_SIZE,\n            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n        )\n        pred_texts = batch_ids_to_strings(pred_ids_batch, vocab, pad_id, eos_id)\n\n        for fn, pred_text in zip(file_names, pred_texts):\n            gt_objects = file_to_objects.get(fn, set())\n            mentioned = find_mentioned_objects(pred_text, category_vocab)\n            if not mentioned:\n                continue\n            total_mentions += len(mentioned)\n            hallucinated = mentioned - gt_objects\n            if hallucinated:\n                caps_with_hallucination += 1\n                hallucinated_mentions += len(hallucinated)\n\n    chair_s = caps_with_hallucination / all_caps if all_caps else 0.0\n    chair_i = hallucinated_mentions / total_mentions if total_mentions else 0.0\n    return {\n        \"CHAIRs\": chair_s,\n        \"CHAIRi\": chair_i,\n        \"total_captions\": all_caps,\n        \"total_mentions\": total_mentions,\n        \"hallucinated_mentions\": hallucinated_mentions,\n    }\n\nchair_stats = compute_chair_on_val_beam(\n    model, val_loader, file_to_objects, category_vocab, vocab, device, max_batches=32\n)\nprint(\"CHAIR stats (beam):\", chair_stats)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# === Cell 11: Show a few beam-search predictions vs ground truth ===\n","metadata":{}},{"cell_type":"code","source":"\nmodel.eval()\nbatch = next(iter(val_loader))\nimages = batch[\"image\"]\ncaptions = batch[\"caption\"]\nfile_names = batch[\"file_name\"]\n\npred_ids_batch = beam_search_decode_batch(\n    model, images, bos_id, eos_id, MAX_LEN, device,\n    beam_size=BEAM_SIZE,\n    no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n)\n\nfor i in range(10):\n    gt_ids   = captions[i].tolist()\n    pred_ids = pred_ids_batch[i].tolist()\n    \n\n    gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n    pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n    print(\"\\nFile:\", file_names[i])\n    print(\"PRED:\", \" \".join(pred_words))\n    print(\"GT  :\", \" \".join(gt_words))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Raw predicted ID sequences for first few examples:\")\nfor i in range(5):\n    print(pred_ids_batch[i].tolist())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# === Cell 11 Updated: Show image + predictions + GT ===\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nmodel.eval()\nbatch = next(iter(val_loader))\nimages = batch[\"image\"]\ncaptions = batch[\"caption\"]\nfile_names = batch[\"file_name\"]\n\npred_ids_batch = beam_search_decode_batch(\n    model, images, bos_id, eos_id, MAX_LEN, device,\n    beam_size=BEAM_SIZE,\n    no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n)\n\n# Show first 10 results with images\nfor i in range(10):\n    img_tensor = images[i].cpu()\n\n    # Un-normalize for viewing (reverse ImageNet normalization)\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n    img_np = (img_tensor * std + mean).clamp(0,1).permute(1,2,0).numpy()\n\n    gt_ids   = captions[i].tolist()\n    pred_ids = pred_ids_batch[i].tolist()\n\n    gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n    pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n    plt.figure(figsize=(5,5))\n    plt.imshow(img_np)\n    plt.axis(\"off\")\n\n    title_str = f\"PRED: {' '.join(pred_words)}\\nGT: {' '.join(gt_words)}\"\n    plt.title(title_str, fontsize=9)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}