{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Run this to download the dataset if not using kaggle**","metadata":{}},{"cell_type":"code","source":"import os\n\n# CHANGE THIS to where you want the dataset\nCOCO_ROOT = \"/kaggle/input/coco-2017-dataset\"  \n\nIMAGES_DIR = COCO_ROOT  # train2017/ and val2017/ will live directly under this\nANN_DIR = os.path.join(COCO_ROOT, \"annotations\")\n\nos.makedirs(COCO_ROOT, exist_ok=True)\nos.makedirs(ANN_DIR, exist_ok=True)\n\nCOCO_ROOT, IMAGES_DIR, ANN_DIR\n\n# This cell uses IPython's ! to run shell commands.\n# It will:\n#  - download train2017.zip\n#  - download val2017.zip\n#  - download annotations_trainval2017.zip\n\nprint(\"Downloading MS COCO 2017 train/val + annotations to\", COCO_ROOT)\n\n# Train images\n!cd \"$COCO_ROOT\" && wget -c http://images.cocodataset.org/zips/train2017.zip\n\n# Val images\n!cd \"$COCO_ROOT\" && wget -c http://images.cocodataset.org/zips/val2017.zip\n\n# Train/Val annotations (includes captions)\n!cd \"$COCO_ROOT\" && wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n\n\n# Unzip train and val images into COCO_ROOT\n!cd \"$COCO_ROOT\" && unzip -q train2017.zip\n!cd \"$COCO_ROOT\" && unzip -q val2017.zip\n\n# Unzip annotations into COCO_ROOT/annotations\n!cd \"$COCO_ROOT\" && unzip -q annotations_trainval2017.zip -d \"$ANN_DIR\"\n\n\nimport glob\nfor z in glob.glob(os.path.join(COCO_ROOT, \"*.zip\")):\n    print(\"Removing\", z)\n    os.remove(z)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport re\nfrom collections import Counter\nfrom typing import List, Dict, Any\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom tqdm.auto import tqdm\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\n# ---- Device ----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')\nprint(\"Using device:\", device)\n\n# ---- Paths (change COCO_ROOT if needed) ----\nCOCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"  # <- adjust if your dataset is elsewhere\nANN_DIR   = os.path.join(COCO_ROOT, \"annotations\")\n\nTRAIN_IMAGES_DIR = os.path.join(COCO_ROOT, \"train2017\")\nVAL_IMAGES_DIR   = os.path.join(COCO_ROOT, \"val2017\")\nTRAIN_JSON       = os.path.join(ANN_DIR, \"captions_train2017.json\")\nVAL_JSON         = os.path.join(ANN_DIR, \"captions_val2017.json\")\n\nprint(\"Train images dir:\", TRAIN_IMAGES_DIR)\nprint(\"Val images dir  :\", VAL_IMAGES_DIR)\nprint(\"Train captions  :\", TRAIN_JSON)\nprint(\"Val captions    :\", VAL_JSON)\n\n# ---- Training config ----\nMAX_LEN      = 30\nFREQ_THRESH  = 5\nBATCH_SIZE   = 32\nDEBUG_LIMIT  = 100000     # use a subset for speed; set to None for full\nVAL_DEBUG    = 5000\n\nWORK_DIR   = \"/kaggle/working\"\nVOCAB_PATH = os.path.join(WORK_DIR, \"vocab_sat.json\")\nos.makedirs(WORK_DIR, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell B: Vocabulary, tokenization, and build vocab ===\n\nSPECIAL_TOKENS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n\nclass Vocabulary:\n    def __init__(self, freq_threshold: int = 5):\n        self.freq_threshold = freq_threshold\n        self.itos: List[str] = []\n        self.stoi: Dict[str, int] = {}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def build_vocab(self, counter: Counter):\n        self.itos = SPECIAL_TOKENS.copy()\n        for word, freq in counter.items():\n            if freq >= self.freq_threshold:\n                self.itos.append(word)\n        self.stoi = {w: i for i, w in enumerate(self.itos)}\n        print(f\"Vocab built: {len(self.itos)} tokens (freq ≥ {self.freq_threshold})\")\n\n    def numericalize(self, tokens: List[str]) -> List[int]:\n        return [self.stoi.get(tok, self.stoi[\"<unk>\"]) for tok in tokens]\n\n    def save(self, path: str):\n        obj = {\"itos\": self.itos, \"freq_threshold\": self.freq_threshold}\n        with open(path, \"w\") as f:\n            json.dump(obj, f)\n        print(f\"Saved vocab to {path}\")\n\n    @classmethod\n    def load(cls, path: str) -> \"Vocabulary\":\n        with open(path, \"r\") as f:\n            obj = json.load(f)\n        vocab = cls(freq_threshold=obj.get(\"freq_threshold\", 5))\n        vocab.itos = obj[\"itos\"]\n        vocab.stoi = {w: i for i, w in enumerate(vocab.itos)}\n        print(f\"Loaded vocab from {path}, size={len(vocab)}\")\n        return vocab\n\ndef tokenize_caption(text: str) -> List[str]:\n    text = text.lower().strip()\n    text = re.sub(r\"[.?!]+$\", \"\", text)\n    return text.split()\n\n# ---- Build or load vocab ----\nif os.path.exists(VOCAB_PATH):\n    vocab = Vocabulary.load(VOCAB_PATH)\nelse:\n    with open(TRAIN_JSON, \"r\") as f:\n        train_ann = json.load(f)\n    print(\"Num training captions:\", len(train_ann[\"annotations\"]))\n\n    counter = Counter()\n    for ann in tqdm(train_ann[\"annotations\"], desc=\"Counting words for vocab\"):\n        tokens = tokenize_caption(ann[\"caption\"])\n        counter.update(tokens)\n\n    vocab = Vocabulary(freq_threshold=FREQ_THRESH)\n    vocab.build_vocab(counter)\n    vocab.save(VOCAB_PATH)\n\npad_id = vocab.stoi[\"<pad>\"]\nbos_id = vocab.stoi[\"<bos>\"]\neos_id = vocab.stoi[\"<eos>\"]\n\nprint(\"pad/bos/eos ids:\", pad_id, bos_id, eos_id)\nprint(\"Vocab size:\", len(vocab))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell C: Dataset and DataLoaders ===\n\nclass COCODataset2017(Dataset):\n    def __init__(\n        self,\n        images_root: str,\n        captions_json: str,\n        vocab: Vocabulary,\n        max_len: int = 30,\n        transform=None,\n        debug_limit: int = None,\n    ):\n        self.images_root = images_root\n        self.vocab = vocab\n        self.max_len = max_len\n        self.transform = transform or self._default_transform()\n\n        with open(captions_json, \"r\") as f:\n            ann = json.load(f)\n\n        self.imgs = {img[\"id\"]: img for img in ann[\"images\"]}\n\n        self.samples = []\n        for a in ann[\"annotations\"]:\n            img_id = a[\"image_id\"]\n            caption = a[\"caption\"]\n            tokens = tokenize_caption(caption)\n            self.samples.append((self.imgs[img_id][\"file_name\"], tokens))\n\n        if debug_limit is not None:\n            self.samples = self.samples[:debug_limit]\n            print(f\"[COCODataset2017] Debug limit: {len(self.samples)} samples\")\n\n        print(f\"Loaded {len(self.samples)} (image, caption) pairs from {captions_json}\")\n\n    def _default_transform(self):\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n        ])\n\n    def __len__(self) -> int:\n        return len(self.samples)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        file_name, tokens = self.samples[idx]\n        img_path = os.path.join(self.images_root, file_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n\n        seq_ids = [bos_id] + self.vocab.numericalize(tokens) + [eos_id]\n        if len(seq_ids) < self.max_len:\n            seq_ids += [pad_id] * (self.max_len - len(seq_ids))\n        else:\n            seq_ids = seq_ids[:self.max_len]\n\n        caption = torch.tensor(seq_ids, dtype=torch.long)\n\n        return {\n            \"image\": image,\n            \"caption\": caption,\n            \"file_name\": file_name,\n        }\n\ndef coco_collate_fn(batch):\n    images = torch.stack([b[\"image\"] for b in batch], dim=0)\n    captions = torch.stack([b[\"caption\"] for b in batch], dim=0)\n    file_names = [b[\"file_name\"] for b in batch]\n    return {\n        \"image\": images,\n        \"caption\": captions,\n        \"file_name\": file_names,\n    }\n\ntrain_dataset = COCODataset2017(\n    images_root=TRAIN_IMAGES_DIR,\n    captions_json=TRAIN_JSON,\n    vocab=vocab,\n    max_len=MAX_LEN,\n    debug_limit=DEBUG_LIMIT,\n)\n\nval_dataset = COCODataset2017(\n    images_root=VAL_IMAGES_DIR,\n    captions_json=VAL_JSON,\n    vocab=vocab,\n    max_len=MAX_LEN,\n    debug_limit=VAL_DEBUG,\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=coco_collate_fn,\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=coco_collate_fn,\n)\n\nprint(\"Train samples:\", len(train_dataset))\nprint(\"Val samples  :\", len(val_dataset))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 2: EncoderCNN (ResNet-based encoder) ===\n\nclass EncoderCNN(nn.Module):\n    \"\"\"\n    CNN encoder for Show, Attend and Tell:\n    - Use a pretrained ResNet (e.g., ResNet-50)\n    - Remove the final pooling + fc\n    - Apply AdaptiveAvgPool2d to get a fixed spatial size\n    - Output shape: [B, enc_image_size, enc_image_size, encoder_dim]\n    \"\"\"\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        # Remove the final fully connected layer & pooling\n        modules = list(resnet.children())[:-2]  # everything until last conv\n        self.cnn = nn.Sequential(*modules)\n\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune(False)\n\n    def forward(self, images):\n        \"\"\"\n        images: [B, 3, 224, 224]\n        returns: [B, enc_image_size, enc_image_size, encoder_dim]\n        \"\"\"\n        out = self.cnn(images)                      # [B, 2048, H, W]\n        out = self.adaptive_pool(out)               # [B, 2048, enc_image_size, enc_image_size]\n        out = out.permute(0, 2, 3, 1)               # [B, enc_image_size, enc_image_size, 2048]\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks.\n        By default we freeze everything for stability; you can unfreeze later.\n        \"\"\"\n        for p in self.cnn.parameters():\n            p.requires_grad = False\n\n        # Unfreeze some layers if fine_tune=True (e.g., last 2 blocks)\n        if fine_tune:\n            for c in list(self.cnn.children())[-2:]:\n                for p in c.parameters():\n                    p.requires_grad = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 3: Attention module and DecoderWithAttention ===\n\nclass Attention(nn.Module):\n    \"\"\"\n    Soft 'additive' attention:\n    Given encoder_out (image features) and decoder hidden state,\n    produce attention weights over image locations and a context vector.\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super().__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # encoder features -> att\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # decoder hidden -> att\n        self.full_att = nn.Linear(attention_dim, 1)               # combine and score\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)                          # over pixels\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        decoder_hidden: [B, decoder_dim]\n        returns:\n            attention_weighted_encoding: [B, encoder_dim]\n            alpha: [B, num_pixels]\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)                       # [B, num_pixels, att_dim]\n        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)       # [B, 1, att_dim]\n        att = self.full_att(self.relu(att1 + att2)).squeeze(2)     # [B, num_pixels]\n        alpha = self.softmax(att)                                  # [B, num_pixels]\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [B, enc_dim]\n        return attention_weighted_encoding, alpha\n\n\nclass DecoderWithAttention(nn.Module):\n    \"\"\"\n    LSTM decoder with attention (Show, Attend and Tell style).\n    We use fixed-length captions with BOS/EOS/PAD.\n    \"\"\"\n    def __init__(\n        self,\n        attention_dim,\n        embed_dim,\n        decoder_dim,\n        vocab_size,\n        encoder_dim=2048,\n        dropout=0.5,\n        max_len=30,\n    ):\n        super().__init__()\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # gating scalar\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n\n        self.max_len = max_len\n        self.vocab_size = vocab_size\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)   # [B, encoder_dim]\n        h = self.init_h(mean_encoder_out)            # [B, decoder_dim]\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions):\n        \"\"\"\n        Forward pass during training (teacher forcing).\n        encoder_out: [B, enc_image_size, enc_image_size, encoder_dim]\n        encoded_captions: [B, T] (with BOS, EOS, PAD)\n        Returns:\n            predictions: [B, T-1, vocab_size]\n            alphas: [B, T-1, num_pixels]\n        We predict tokens for positions 1..T-1 (targets are 1..T, i.e. shifted).\n        \"\"\"\n        B = encoder_out.size(0)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(-1)\n        num_pixels = enc_image_size * enc_image_size\n\n        # Flatten spatial dims\n        encoder_out = encoder_out.view(B, -1, encoder_dim)  # [B, num_pixels, encoder_dim]\n\n        # Prepare embeddings (we'll ignore the last token when feeding)\n        embeddings = self.embedding(encoded_captions)       # [B, T, embed_dim]\n        T = encoded_captions.size(1)\n\n        h, c = self.init_hidden_state(encoder_out)\n\n        preds = []\n        alphas = []\n\n        # Decode from t=0..T-2 (We use caption[t] as input, target is caption[t+1])\n        for t in range(T - 1):\n            batch_emb_t = embeddings[:, t, :]                # [B, embed_dim]\n            context, alpha = self.attention(encoder_out, h)  # [B, enc_dim], [B, num_pixels]\n            gate = self.sigmoid(self.f_beta(h))              # gating [B, enc_dim]\n            context = gate * context\n\n            # LSTMCell input is [embed, context]\n            lstm_input = torch.cat([batch_emb_t, context], dim=1)  # [B, embed_dim+enc_dim]\n            h, c = self.decode_step(lstm_input, (h, c))            # both [B, decoder_dim]\n\n            output = self.fc(self.dropout(h))                      # [B, vocab_size]\n            preds.append(output)\n            alphas.append(alpha)\n\n        preds = torch.stack(preds, dim=1)   # [B, T-1, vocab_size]\n        alphas = torch.stack(alphas, dim=1) # [B, T-1, num_pixels]\n\n        return preds, alphas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 4: EncoderCNN, Attention, DecoderWithAttention, SAT wrapper ===\n\nclass EncoderCNN(nn.Module):\n    \"\"\"\n    CNN encoder: ResNet-50, output spatial feature map.\n    \"\"\"\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        modules = list(resnet.children())[:-2]  # remove avgpool & fc\n        self.cnn = nn.Sequential(*modules)\n\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune(False)\n\n    def forward(self, images):\n        \"\"\"\n        images: [B, 3, 224, 224]\n        returns: [B, enc_image_size, enc_image_size, encoder_dim]\n        \"\"\"\n        out = self.cnn(images)                      # [B, 2048, H, W]\n        out = self.adaptive_pool(out)               # [B, 2048, enc_image_size, enc_image_size]\n        out = out.permute(0, 2, 3, 1)               # [B, enc_image_size, enc_image_size, 2048]\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        \"\"\"\n        Unfreeze last conv blocks if fine_tune=True.\n        \"\"\"\n        for p in self.cnn.parameters():\n            p.requires_grad = False\n\n        if fine_tune:\n            for c in list(self.cnn.children())[-2:]:\n                for p in c.parameters():\n                    p.requires_grad = True\n\nclass Attention(nn.Module):\n    \"\"\"\n    Additive attention over spatial image features.\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super().__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att    = nn.Linear(attention_dim, 1)\n        self.relu        = nn.ReLU()\n        self.softmax     = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        decoder_hidden: [B, decoder_dim]\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)             # [B, num_pixels, att_dim]\n        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [B, 1, att_dim]\n        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [B, num_pixels]\n        alpha = self.softmax(att)                        # [B, num_pixels]\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [B, enc_dim]\n        return attention_weighted_encoding, alpha\n\nclass DecoderWithAttention(nn.Module):\n    \"\"\"\n    LSTM decoder with attention (Show, Attend and Tell).\n    \"\"\"\n    def __init__(\n        self,\n        attention_dim,\n        embed_dim,\n        decoder_dim,\n        vocab_size,\n        encoder_dim=2048,\n        dropout=0.5,\n        max_len=30,\n    ):\n        super().__init__()\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout   = nn.Dropout(dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n\n        self.max_len    = max_len\n        self.vocab_size = vocab_size\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)  # [B, encoder_dim]\n        h = self.init_h(mean_encoder_out)           # [B, decoder_dim]\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions):\n        \"\"\"\n        encoder_out: [B, H, W, encoder_dim]\n        encoded_captions: [B, T]  (with <bos>, ..., <eos>/<pad>)\n        Returns:\n            preds:  [B, T-1, vocab_size]\n            alphas: [B, T-1, num_pixels]\n        \"\"\"\n        B = encoder_out.size(0)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(-1)\n        num_pixels = enc_image_size * enc_image_size\n\n        # Flatten spatial dims\n        encoder_out = encoder_out.view(B, -1, encoder_dim)  # [B, num_pixels, encoder_dim]\n\n        # Embeddings\n        embeddings = self.embedding(encoded_captions)       # [B, T, embed_dim]\n        T = encoded_captions.size(1)\n\n        h, c = self.init_hidden_state(encoder_out)\n\n        preds = []\n        alphas = []\n\n        # Teacher forcing: input caption[t], target caption[t+1]\n        for t in range(T - 1):\n            batch_emb_t = embeddings[:, t, :]                # [B, embed_dim]\n            context, alpha = self.attention(encoder_out, h)  # [B, enc_dim], [B, num_pixels]\n            gate = self.sigmoid(self.f_beta(h))              # [B, enc_dim]\n            context = gate * context\n\n            lstm_input = torch.cat([batch_emb_t, context], dim=1)  # [B, embed_dim + enc_dim]\n            h, c = self.decode_step(lstm_input, (h, c))            # [B, decoder_dim]\n\n            output = self.fc(self.dropout(h))                      # [B, vocab_size]\n            preds.append(output)\n            alphas.append(alpha)\n\n        preds = torch.stack(preds, dim=1)    # [B, T-1, vocab_size]\n        alphas = torch.stack(alphas, dim=1)  # [B, T-1, num_pixels]\n\n        return preds, alphas\n\nclass ShowAttendTell(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        attention_dim=512,\n        embed_dim=512,\n        decoder_dim=512,\n        encoder_dim=2048,\n        dropout=0.5,\n        max_len=30,\n    ):\n        super().__init__()\n        self.encoder = EncoderCNN()\n        self.decoder = DecoderWithAttention(\n            attention_dim=attention_dim,\n            embed_dim=embed_dim,\n            decoder_dim=decoder_dim,\n            vocab_size=vocab_size,\n            encoder_dim=encoder_dim,\n            dropout=dropout,\n            max_len=max_len,\n        )\n\n    def forward(self, images, captions):\n        encoder_out = self.encoder(images)\n        preds, alphas = self.decoder(encoder_out, captions)\n        return preds, alphas\n\nmodel_sat = ShowAttendTell(\n    vocab_size=len(vocab),\n    attention_dim=512,\n    embed_dim=512,\n    decoder_dim=512,\n    encoder_dim=2048,\n    dropout=0.5,\n    max_len=MAX_LEN,\n).to(device)\n\ncriterion_sat = nn.CrossEntropyLoss(ignore_index=pad_id)\noptimizer_sat = torch.optim.AdamW(\n    filter(lambda p: p.requires_grad, model_sat.parameters()),\n    lr=3e-4,\n    weight_decay=1e-4,\n)\n\nprint(\"Trainable params:\",\n      sum(p.numel() for p in model_sat.parameters() if p.requires_grad))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 5: Training loop for SAT (CE only) ===\n\ndef train_one_epoch_sat(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n\n    for batch in tqdm(loader, desc=\"SAT train (1 epoch)\"):\n        images = batch[\"image\"].to(device)\n        captions = batch[\"caption\"].to(device)\n\n        preds, alphas = model(images, captions)      # preds: [B, T-1, V]\n        B, Tm1, V = preds.size()\n        targets = captions[:, 1:]                    # [B, T-1]\n\n        loss = criterion(\n            preds.reshape(B * Tm1, V),\n            targets.reshape(B * Tm1),\n        )\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\nEPOCHS_SAT = 5  # start small; you can increase later\n\nfor e in range(EPOCHS_SAT):\n    avg_loss = train_one_epoch_sat(model_sat, train_loader, optimizer_sat, criterion_sat, device)\n    print(f\"[SAT] Epoch {e+1}/{EPOCHS_SAT} - loss: {avg_loss:.4f}\")\n    torch.save(model_sat.state_dict(), os.path.join(WORK_DIR, f\"sat_epoch{e+1}.pt\"))\n    print(\"Saved checkpoint:\", f\"sat_epoch{e+1}.pt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 6: Greedy decoding for qualitative check ===\n\ndef ids_to_words(ids, vocab, pad_id, eos_id, drop_bos=True):\n    words = []\n    for idx in ids:\n        if idx == pad_id:\n            break\n        tok = vocab.itos[idx]\n        if drop_bos and tok == \"<bos>\":\n            continue\n        if tok == \"<eos>\":\n            break\n        words.append(tok)\n    return words\n\n@torch.no_grad()\ndef sat_greedy_decode(model, image, max_len=30):\n    \"\"\"\n    Greedy decoding for a single image tensor [3, 224, 224].\n    Returns list of token ids (includes <bos>, ...).\n    \"\"\"\n    model.eval()\n    image = image.unsqueeze(0).to(device)    # [1, 3, 224, 224]\n\n    encoder_out = model.encoder(image)       # [1, H, W, enc_dim]\n    B, H, W, enc_dim = encoder_out.shape\n    num_pixels = H * W\n    encoder_out = encoder_out.view(1, num_pixels, enc_dim)   # [1, num_pixels, enc_dim]\n\n    h, c = model.decoder.init_hidden_state(encoder_out)\n\n    seq = [bos_id]\n    prev_word = torch.tensor([bos_id], device=device, dtype=torch.long)\n    prev_emb = model.decoder.embedding(prev_word)            # [1, embed_dim]\n\n    for _ in range(max_len - 1):\n        context, alpha = model.decoder.attention(encoder_out, h)\n        gate = model.decoder.sigmoid(model.decoder.f_beta(h))\n        context = gate * context\n\n        lstm_input = torch.cat([prev_emb, context], dim=1)\n        h, c = model.decoder.decode_step(lstm_input, (h, c))\n\n        output = model.decoder.fc(model.decoder.dropout(h))  # [1, vocab_size]\n        _, next_word = output.max(dim=1)\n        next_id = next_word.item()\n        seq.append(next_id)\n\n        if next_id == eos_id:\n            break\n\n        prev_emb = model.decoder.embedding(next_word)\n\n    return seq\n\n# --- Test on a small batch from val_loader ---\nbatch = next(iter(val_loader))\nimages = batch[\"image\"]\ncaptions = batch[\"caption\"]\nfile_names = batch[\"file_name\"]\n\nfor i in range(10):\n    img = images[i]\n    gt_ids = captions[i].tolist()\n    pred_ids = sat_greedy_decode(model_sat, img, max_len=MAX_LEN)\n\n    gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n    pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n    print(\"\\nFile:\", file_names[i])\n    print(\"PRED:\", \" \".join(pred_words) if pred_words else \"[EMPTY]\")\n    print(\"GT  :\", \" \".join(gt_words))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 7: Beam search decoding for Show, Attend and Tell ===\n\nimport math\n\n@torch.no_grad()\ndef sat_beam_search_single(\n    model,\n    image,          # [3, 224, 224]\n    bos_id,\n    eos_id,\n    pad_id,\n    beam_size=3,\n    max_len=30,\n):\n    \"\"\"\n    Beam search for a single image.\n    Returns: list of token ids (including <bos>, ..., <eos>).\n    \"\"\"\n    model.eval()\n\n    # Encode image\n    img = image.unsqueeze(0).to(device)           # [1, 3, H, W]\n    encoder_out = model.encoder(img)              # [1, H, W, enc_dim]\n    B, H, W, enc_dim = encoder_out.shape\n    num_pixels = H * W\n    encoder_out = encoder_out.view(1, num_pixels, enc_dim)  # [1, num_pixels, enc_dim]\n\n    # Init hidden state\n    h, c = model.decoder.init_hidden_state(encoder_out)     # [1, dec_dim]\n\n    # Beam: list of (log_prob, seq, h, c)\n    # seq is a list of token ids\n    start_seq = [bos_id]\n    beam = [(0.0, start_seq, h, c)]   # log_prob = 0\n\n    completed = []\n\n    for _ in range(max_len - 1):\n        new_beam = []\n\n        for log_p, seq, h_prev, c_prev in beam:\n            # If already ended, keep in completed\n            if seq[-1] == eos_id:\n                completed.append((log_p, seq))\n                continue\n\n            prev_word_id = seq[-1]\n            prev_word = torch.tensor([prev_word_id], device=device, dtype=torch.long)\n            prev_emb = model.decoder.embedding(prev_word)             # [1, embed_dim]\n\n            # Attention\n            context, alpha = model.decoder.attention(encoder_out, h_prev)\n            gate = model.decoder.sigmoid(model.decoder.f_beta(h_prev))\n            context = gate * context\n\n            lstm_input = torch.cat([prev_emb, context], dim=1)        # [1, emb+enc]\n            h_new, c_new = model.decoder.decode_step(lstm_input, (h_prev, c_prev))\n\n            output = model.decoder.fc(model.decoder.dropout(h_new))   # [1, vocab_size]\n            log_probs = torch.log_softmax(output, dim=-1).squeeze(0)  # [vocab_size]\n\n            # Top-k expansions\n            topk_logp, topk_ids = torch.topk(log_probs, beam_size)\n\n            for lp, idx in zip(topk_logp.tolist(), topk_ids.tolist()):\n                new_seq = seq + [idx]\n                new_log_p = log_p + lp\n                new_beam.append((new_log_p, new_seq, h_new, c_new))\n\n        if not new_beam:\n            break\n\n        # Keep top beam_size\n        new_beam.sort(key=lambda x: x[0], reverse=True)\n        beam = new_beam[:beam_size]\n\n        # If we already have enough completed sequences and all beams ended, break\n        all_ended = all(seq[-1] == eos_id for _, seq, _, _ in beam)\n        if all_ended:\n            break\n\n    # If we found completed sequences, pick the best\n    if completed:\n        completed.sort(key=lambda x: x[0], reverse=True)\n        best_logp, best_seq = completed[0]\n    else:\n        # Otherwise, use the best in the current beam\n        beam.sort(key=lambda x: x[0], reverse=True)\n        best_logp, best_seq, _, _ = beam[0]\n\n    return best_seq\n\n@torch.no_grad()\ndef sat_beam_search_batch(\n    model,\n    images,     # [B, 3, H, W]\n    bos_id,\n    eos_id,\n    pad_id,\n    beam_size=3,\n    max_len=30,\n):\n    \"\"\"\n    Run beam search for each image in a batch independently.\n    Returns: list of lists of ids.\n    \"\"\"\n    preds = []\n    B = images.size(0)\n    for i in range(B):\n        seq = sat_beam_search_single(\n            model,\n            images[i],\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=max_len,\n        )\n        preds.append(seq)\n    return preds\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick sanity: compare greedy vs beam for a few val images\nbatch = next(iter(val_loader))\nimages = batch[\"image\"]\ncaptions = batch[\"caption\"]\nfile_names = batch[\"file_name\"]\n\nfor i in range(10):\n    img = images[i]\n    gt_ids = captions[i].tolist()\n\n    greedy_ids = sat_greedy_decode(model_sat, img, max_len=MAX_LEN)\n    beam_ids   = sat_beam_search_single(model_sat, img, bos_id, eos_id, pad_id, beam_size=3, max_len=MAX_LEN)\n\n    greedy_words = ids_to_words(greedy_ids, vocab, pad_id, eos_id)\n    beam_words   = ids_to_words(beam_ids, vocab, pad_id, eos_id)\n    gt_words     = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n\n    print(\"\\nFile:\", file_names[i])\n    print(\"GREEDY:\", \" \".join(greedy_words))\n    print(\"BEAM  :\", \" \".join(beam_words))\n    print(\"GT    :\", \" \".join(gt_words))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 8: BLEU-4 evaluation (beam search) ===\n\n!pip install nltk -q\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nnltk.download('punkt', quiet=True)\n\ndef evaluate_bleu_sat_beam(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=50,   # for speed; set to len(loader) for full\n):\n    model.eval()\n    smoothie = SmoothingFunction().method4\n    scores = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"BLEU eval (SAT beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        captions = batch[\"caption\"]\n\n        pred_ids_batch = sat_beam_search_batch(\n            model,\n            images,\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=MAX_LEN,\n        )\n\n        B = captions.size(0)\n        for i in range(B):\n            gt_ids   = captions[i].tolist()\n            pred_ids = pred_ids_batch[i]\n\n            gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n            pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n            if len(pred_words) == 0 or len(gt_words) == 0:\n                continue\n\n            score = sentence_bleu(\n                [gt_words],\n                pred_words,\n                smoothing_function=smoothie,\n                weights=(0.25, 0.25, 0.25, 0.25),\n            )\n            scores.append(score)\n\n    if not scores:\n        return 0.0\n    return float(np.mean(scores))\n\nbleu_beam = evaluate_bleu_sat_beam(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=300,   # ~30 batches for a quick estimate\n)\n\nprint(\"Approx BLEU-4 (SAT + beam=3):\", bleu_beam)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 9: CLIPScore-style evaluation for SAT (beam captions) ===\n\n!pip install open_clip_torch -q\nimport open_clip\nfrom PIL import Image\n\n# Load CLIP model + preprocess once\nclip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n    \"ViT-B-32\",\n    pretrained=\"openai\",\n)\nclip_model = clip_model.to(device).eval()\nclip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n\n@torch.no_grad()\ndef compute_clipscore_sat_beam(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=10,  # small for speed; increase if you want\n):\n    sims = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CLIPScore eval (SAT beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]   # [B, 3, H, W]\n        file_names = batch[\"file_name\"]\n\n        # Decode captions with beam search\n        pred_ids_batch = sat_beam_search_batch(\n            model,\n            images,\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=MAX_LEN,\n        )\n        pred_texts = []\n        for seq in pred_ids_batch:\n            pred_texts.append(\" \".join(ids_to_words(seq, vocab, pad_id, eos_id)))\n\n        # For each image–caption pair, compute CLIP similarity\n        for img_tensor, cap in zip(images, pred_texts):\n            # Convert tensor -> PIL, apply CLIP preprocess\n            pil = transforms.ToPILImage()(img_tensor)\n            image_input = clip_preprocess(pil).unsqueeze(0).to(device)\n            text_tokens = clip_tokenizer([cap]).to(device)\n\n            img_feat = clip_model.encode_image(image_input)\n            txt_feat = clip_model.encode_text(text_tokens)\n\n            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n            txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n            sim = (img_feat * txt_feat).sum(dim=-1).item()\n            sims.append(sim)\n\n    sims = np.array(sims)\n    stats = {\n        \"mean\": float(sims.mean()),\n        \"median\": float(np.median(sims)),\n        \"std\": float(sims.std()),\n        \"low_clip_rate\": float((sims < 0.2).mean()),\n    }\n    return stats\n\nclip_stats_sat = compute_clipscore_sat_beam(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=10,  # just a sample for speed\n)\nprint(\"CLIPScore stats (SAT + beam=3):\", clip_stats_sat)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 10: CHAIR hallucination metrics for SAT (beam captions) ===\n\n# 1) Load instance annotations (COCO 2017 val)\nINST_VAL_JSON = os.path.join(ANN_DIR, \"instances_val2017.json\")\nwith open(INST_VAL_JSON, \"r\") as f:\n    inst_val = json.load(f)\n\n# Map image_id -> file_name and category_id -> name\nimgid_to_file = {img[\"id\"]: img[\"file_name\"] for img in inst_val[\"images\"]}\ncatid_to_name = {c[\"id\"]: c[\"name\"].lower() for c in inst_val[\"categories\"]}\n\n# Build mapping from file_name -> set of ground-truth object classes\nfrom collections import defaultdict\nfile_to_objects = defaultdict(set)\nfor ann in inst_val[\"annotations\"]:\n    img_id = ann[\"image_id\"]\n    cat_id = ann[\"category_id\"]\n    file_name = imgid_to_file[img_id]\n    cat_name = catid_to_name[cat_id]\n    file_to_objects[file_name].add(cat_name)\n\n# Category vocab for matching words in captions\ndef simple_tokenize(text: str):\n    return re.findall(r\"[a-z]+\", text.lower())\n\ncategory_vocab = []\nfor cat_name in sorted(set(catid_to_name.values())):\n    tokens = simple_tokenize(cat_name)\n    category_vocab.append((cat_name, tokens))\n\ndef find_mentioned_objects(caption_text: str, category_vocab):\n    tokens = simple_tokenize(caption_text)\n    mentioned = set()\n    for cat_name, cat_tokens in category_vocab:\n        L = len(cat_tokens)\n        if L == 1:\n            if cat_tokens[0] in tokens:\n                mentioned.add(cat_name)\n        else:\n            # multi-word categories like \"traffic light\"\n            for i in range(len(tokens) - L + 1):\n                if tokens[i:i+L] == cat_tokens:\n                    mentioned.add(cat_name)\n                    break\n    return mentioned\n\n@torch.no_grad()\ndef compute_chair_sat_beam(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    beam_size=3,\n    max_batches=30,\n):\n    model.eval()\n    all_caps = 0\n    caps_with_hallucination = 0\n    total_mentions = 0\n    hallucinated_mentions = 0\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CHAIR eval (SAT beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        file_names = batch[\"file_name\"]\n\n        pred_ids_batch = sat_beam_search_batch(\n            model,\n            images,\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=MAX_LEN,\n        )\n        pred_texts = [\n            \" \".join(ids_to_words(seq, vocab, pad_id, eos_id))\n            for seq in pred_ids_batch\n        ]\n\n        for fn, cap in zip(file_names, pred_texts):\n            all_caps += 1\n            gt_objects = file_to_objects.get(fn, set())\n            mentioned = find_mentioned_objects(cap, category_vocab)\n            if not mentioned:\n                continue\n\n            total_mentions += len(mentioned)\n            hallucinated = mentioned - gt_objects\n            if hallucinated:\n                caps_with_hallucination += 1\n                hallucinated_mentions += len(hallucinated)\n\n    chair_s = caps_with_hallucination / all_caps if all_caps else 0.0\n    chair_i = hallucinated_mentions / total_mentions if total_mentions else 0.0\n    return {\n        \"CHAIRs\": chair_s,\n        \"CHAIRi\": chair_i,\n        \"total_captions\": all_caps,\n        \"total_mentions\": total_mentions,\n        \"hallucinated_mentions\": hallucinated_mentions,\n    }\n\nchair_stats_sat = compute_chair_sat_beam(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    beam_size=3,\n    max_batches=32,\n)\nprint(\"CHAIR stats (SAT + beam=3):\", chair_stats_sat)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === NEW CELL: BLEU-4 evaluation (SAT + greedy) ===\n\ndef evaluate_bleu_sat_greedy(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=50,   # for speed; set to len(loader) for full\n):\n    model.eval()\n    smoothie = SmoothingFunction().method4\n    scores = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"BLEU eval (SAT greedy)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        captions = batch[\"caption\"]\n\n        B = images.size(0)\n        for i in range(B):\n            img = images[i]\n            gt_ids = captions[i].tolist()\n\n            # Greedy decoding for this image\n            pred_ids = sat_greedy_decode(model, img, max_len=MAX_LEN)\n\n            gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n            pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n            if len(pred_words) == 0 or len(gt_words) == 0:\n                continue\n\n            score = sentence_bleu(\n                [gt_words],\n                pred_words,\n                smoothing_function=smoothie,\n                weights=(0.25, 0.25, 0.25, 0.25),\n            )\n            scores.append(score)\n\n    if not scores:\n        return 0.0\n    return float(np.mean(scores))\n\nbleu_greedy = evaluate_bleu_sat_greedy(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=300,  # adjust as you like\n)\n\nprint(\"Approx BLEU-4 (SAT + greedy):\", bleu_greedy)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === NEW CELL: CLIPScore evaluation (SAT + greedy) ===\n\n@torch.no_grad()\ndef compute_clipscore_sat_greedy(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=10,  # small for speed; increase if needed\n):\n    model.eval()\n    sims = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CLIPScore eval (SAT greedy)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]   # [B, 3, H, W]\n\n        B = images.size(0)\n        for i in range(B):\n            img_tensor = images[i]\n\n            # Greedy caption\n            pred_ids = sat_greedy_decode(model, img_tensor, max_len=MAX_LEN)\n            caption  = \" \".join(ids_to_words(pred_ids, vocab, pad_id, eos_id))\n\n            # Convert tensor -> PIL, apply CLIP preprocess\n            pil = transforms.ToPILImage()(img_tensor)\n            image_input = clip_preprocess(pil).unsqueeze(0).to(device)\n            text_tokens = clip_tokenizer([caption]).to(device)\n\n            img_feat = clip_model.encode_image(image_input)\n            txt_feat = clip_model.encode_text(text_tokens)\n\n            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n            txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n            sim = (img_feat * txt_feat).sum(dim=-1).item()\n            sims.append(sim)\n\n    sims = np.array(sims)\n    stats = {\n        \"mean\": float(sims.mean()),\n        \"median\": float(np.median(sims)),\n        \"std\": float(sims.std()),\n        \"low_clip_rate\": float((sims < 0.2).mean()),\n    }\n    return stats\n\nclip_stats_sat_greedy = compute_clipscore_sat_greedy(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=32,\n)\nprint(\"CLIPScore stats (SAT + greedy):\", clip_stats_sat_greedy)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === NEW CELL: CHAIR hallucination metrics (SAT + greedy) ===\n\n@torch.no_grad()\ndef compute_chair_sat_greedy(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    max_batches=30,\n):\n    model.eval()\n    all_caps = 0\n    caps_with_hallucination = 0\n    total_mentions = 0\n    hallucinated_mentions = 0\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CHAIR eval (SAT greedy)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        file_names = batch[\"file_name\"]\n\n        B = images.size(0)\n        for i in range(B):\n            img = images[i]\n            fn  = file_names[i]\n\n            pred_ids = sat_greedy_decode(model, img, max_len=MAX_LEN)\n            cap_text = \" \".join(ids_to_words(pred_ids, vocab, pad_id, eos_id))\n\n            all_caps += 1\n            gt_objects = file_to_objects.get(fn, set())\n            mentioned = find_mentioned_objects(cap_text, category_vocab)\n            if not mentioned:\n                continue\n\n            total_mentions += len(mentioned)\n            hallucinated = mentioned - gt_objects\n            if hallucinated:\n                caps_with_hallucination += 1\n                hallucinated_mentions += len(hallucinated)\n\n    chair_s = caps_with_hallucination / all_caps if all_caps else 0.0\n    chair_i = hallucinated_mentions / total_mentions if total_mentions else 0.0\n    return {\n        \"CHAIRs\": chair_s,\n        \"CHAIRi\": chair_i,\n        \"total_captions\": all_caps,\n        \"total_mentions\": total_mentions,\n        \"hallucinated_mentions\": hallucinated_mentions,\n    }\n\nchair_stats_sat_greedy = compute_chair_sat_greedy(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    max_batches=32,\n)\nprint(\"CHAIR stats (SAT + greedy):\", chair_stats_sat_greedy)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell: Visualize SAT predictions (greedy + beam) with images ===\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# If you normalized images with ImageNet stats, define unnormalize:\nIMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\nIMAGENET_STD  = np.array([0.229, 0.224, 0.225])\n\ndef unnormalize(img_tensor):\n    \"\"\"\n    img_tensor: [3, H, W] tensor in normalized space.\n    Returns HxWx3 numpy array in [0,1].\n    \"\"\"\n    img = img_tensor.cpu().numpy()\n    img = (img * IMAGENET_STD[:, None, None]) + IMAGENET_MEAN[:, None, None]\n    img = np.clip(img, 0.0, 1.0)\n    img = np.transpose(img, (1, 2, 0))  # CHW -> HWC\n    return img\n\ndef decode_ids_to_text(ids, vocab, pad_id, eos_id):\n    return \" \".join(ids_to_words(ids, vocab, pad_id, eos_id))\n\n@torch.no_grad()\ndef show_sat_examples(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    num_examples=5,\n    use_beam=True,\n    beam_size=3,\n    max_len=30,\n):\n    \"\"\"\n    Show a few validation images with:\n      - ground truth caption\n      - SAT greedy prediction\n      - SAT beam prediction (optional)\n    \"\"\"\n    model.eval()\n    \n    batch = next(iter(loader))\n    images = batch[\"image\"]\n    captions = batch[\"caption\"]\n    file_names = batch.get(\"file_name\", None)\n\n    n = min(num_examples, images.size(0))\n\n    for i in range(n):\n        img_tensor = images[i]\n        gt_ids = captions[i].tolist()\n\n        # Greedy prediction\n        pred_ids_greedy = sat_greedy_decode(\n            model,\n            img_tensor,\n            max_len=max_len\n        )\n\n        # Beam prediction (optional)\n        if use_beam:\n            pred_ids_beam = sat_beam_search_single(\n                model,\n                img_tensor,\n                bos_id,\n                eos_id,\n                pad_id,\n                beam_size=beam_size,\n                max_len=max_len,\n            )\n        else:\n            pred_ids_beam = None\n\n        gt_text       = decode_ids_to_text(gt_ids, vocab, pad_id, eos_id)\n        pred_text_gr  = decode_ids_to_text(pred_ids_greedy, vocab, pad_id, eos_id)\n        pred_text_beam = decode_ids_to_text(pred_ids_beam, vocab, pad_id, eos_id) if pred_ids_beam else \"[disabled]\"\n\n        # Plot image\n        plt.figure(figsize=(6, 6))\n        plt.imshow(unnormalize(img_tensor))\n        plt.axis(\"off\")\n\n        title = f\"Example {i+1}\"\n        if file_names is not None:\n            title += f\"  ({file_names[i]})\"\n        plt.title(title, fontsize=12)\n\n        # Print captions under the image\n        print(\"=\" * 80)\n        print(title)\n        print(\"GT      :\", gt_text)\n        print(\"Greedy  :\", pred_text_gr if pred_text_gr else \"[EMPTY]\")\n        if use_beam:\n            print(f\"Beam({beam_size}):\", pred_text_beam if pred_text_beam else \"[EMPTY]\")\n\n        plt.show()\n\n\n# --- Call it (SAT model) ---\nshow_sat_examples(\n    model=model_sat,\n    loader=val_loader,\n    vocab=vocab,\n    pad_id=pad_id,\n    eos_id=eos_id,\n    bos_id=bos_id,\n    device=device,\n    num_examples=10,\n    use_beam=True,   # set False if you only want greedy\n    beam_size=3,\n    max_len=MAX_LEN,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}