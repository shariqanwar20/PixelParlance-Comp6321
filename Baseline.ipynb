{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport re\nfrom collections import Counter\nfrom typing import List, Dict, Any\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom tqdm.auto import tqdm\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\n# ---- Device ----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')\nprint(\"Using device:\", device)\n\n# ---- Paths (change COCO_ROOT if needed) ----\nCOCO_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"  # <- adjust if your dataset is elsewhere\nANN_DIR   = os.path.join(COCO_ROOT, \"annotations\")\n\nTRAIN_IMAGES_DIR = os.path.join(COCO_ROOT, \"train2017\")\nVAL_IMAGES_DIR   = os.path.join(COCO_ROOT, \"val2017\")\nTRAIN_JSON       = os.path.join(ANN_DIR, \"captions_train2017.json\")\nVAL_JSON         = os.path.join(ANN_DIR, \"captions_val2017.json\")\n\nprint(\"Train images dir:\", TRAIN_IMAGES_DIR)\nprint(\"Val images dir  :\", VAL_IMAGES_DIR)\nprint(\"Train captions  :\", TRAIN_JSON)\nprint(\"Val captions    :\", VAL_JSON)\n\n# ---- Training config ----\nMAX_LEN      = 30\nFREQ_THRESH  = 5\nBATCH_SIZE   = 32\nDEBUG_LIMIT  = 10000     # use a subset for speed; set to None for full\nVAL_DEBUG    = 1000\n\nWORK_DIR   = \"/kaggle/working\"\nVOCAB_PATH = os.path.join(WORK_DIR, \"vocab_sat.json\")\nos.makedirs(WORK_DIR, exist_ok=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:23:15.997502Z","iopub.execute_input":"2025-12-08T02:23:15.997805Z","iopub.status.idle":"2025-12-08T02:23:25.664094Z","shell.execute_reply.started":"2025-12-08T02:23:15.997785Z","shell.execute_reply":"2025-12-08T02:23:25.662889Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cpu\nTrain images dir: /kaggle/input/coco-2017-dataset/coco2017/train2017\nVal images dir  : /kaggle/input/coco-2017-dataset/coco2017/val2017\nTrain captions  : /kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json\nVal captions    : /kaggle/input/coco-2017-dataset/coco2017/annotations/captions_val2017.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === Cell B: Vocabulary, tokenization, and build vocab ===\n\nSPECIAL_TOKENS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n\nclass Vocabulary:\n    def __init__(self, freq_threshold: int = 5):\n        self.freq_threshold = freq_threshold\n        self.itos: List[str] = []\n        self.stoi: Dict[str, int] = {}\n\n    def __len__(self):\n        return len(self.itos)\n\n    def build_vocab(self, counter: Counter):\n        self.itos = SPECIAL_TOKENS.copy()\n        for word, freq in counter.items():\n            if freq >= self.freq_threshold:\n                self.itos.append(word)\n        self.stoi = {w: i for i, w in enumerate(self.itos)}\n        print(f\"Vocab built: {len(self.itos)} tokens (freq ≥ {self.freq_threshold})\")\n\n    def numericalize(self, tokens: List[str]) -> List[int]:\n        return [self.stoi.get(tok, self.stoi[\"<unk>\"]) for tok in tokens]\n\n    def save(self, path: str):\n        obj = {\"itos\": self.itos, \"freq_threshold\": self.freq_threshold}\n        with open(path, \"w\") as f:\n            json.dump(obj, f)\n        print(f\"Saved vocab to {path}\")\n\n    @classmethod\n    def load(cls, path: str) -> \"Vocabulary\":\n        with open(path, \"r\") as f:\n            obj = json.load(f)\n        vocab = cls(freq_threshold=obj.get(\"freq_threshold\", 5))\n        vocab.itos = obj[\"itos\"]\n        vocab.stoi = {w: i for i, w in enumerate(vocab.itos)}\n        print(f\"Loaded vocab from {path}, size={len(vocab)}\")\n        return vocab\n\ndef tokenize_caption(text: str) -> List[str]:\n    text = text.lower().strip()\n    text = re.sub(r\"[.?!]+$\", \"\", text)\n    return text.split()\n\n# ---- Build or load vocab ----\nif os.path.exists(VOCAB_PATH):\n    vocab = Vocabulary.load(VOCAB_PATH)\nelse:\n    with open(TRAIN_JSON, \"r\") as f:\n        train_ann = json.load(f)\n    print(\"Num training captions:\", len(train_ann[\"annotations\"]))\n\n    counter = Counter()\n    for ann in tqdm(train_ann[\"annotations\"], desc=\"Counting words for vocab\"):\n        tokens = tokenize_caption(ann[\"caption\"])\n        counter.update(tokens)\n\n    vocab = Vocabulary(freq_threshold=FREQ_THRESH)\n    vocab.build_vocab(counter)\n    vocab.save(VOCAB_PATH)\n\npad_id = vocab.stoi[\"<pad>\"]\nbos_id = vocab.stoi[\"<bos>\"]\neos_id = vocab.stoi[\"<eos>\"]\n\nprint(\"pad/bos/eos ids:\", pad_id, bos_id, eos_id)\nprint(\"Vocab size:\", len(vocab))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:23:34.770698Z","iopub.execute_input":"2025-12-08T02:23:34.771216Z","iopub.status.idle":"2025-12-08T02:23:38.482688Z","shell.execute_reply.started":"2025-12-08T02:23:34.771181Z","shell.execute_reply":"2025-12-08T02:23:38.481537Z"}},"outputs":[{"name":"stdout","text":"Num training captions: 591753\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Counting words for vocab:   0%|          | 0/591753 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f641aae79c5d4d9fb96dc13e25acfc96"}},"metadata":{}},{"name":"stdout","text":"Vocab built: 11405 tokens (freq ≥ 5)\nSaved vocab to /kaggle/working/vocab_sat.json\npad/bos/eos ids: 0 1 2\nVocab size: 11405\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === Cell C: Dataset and DataLoaders ===\n\nclass COCODataset2017(Dataset):\n    def __init__(\n        self,\n        images_root: str,\n        captions_json: str,\n        vocab: Vocabulary,\n        max_len: int = 30,\n        transform=None,\n        debug_limit: int = None,\n    ):\n        self.images_root = images_root\n        self.vocab = vocab\n        self.max_len = max_len\n        self.transform = transform or self._default_transform()\n\n        with open(captions_json, \"r\") as f:\n            ann = json.load(f)\n\n        self.imgs = {img[\"id\"]: img for img in ann[\"images\"]}\n\n        self.samples = []\n        for a in ann[\"annotations\"]:\n            img_id = a[\"image_id\"]\n            caption = a[\"caption\"]\n            tokens = tokenize_caption(caption)\n            self.samples.append((self.imgs[img_id][\"file_name\"], tokens))\n\n        if debug_limit is not None:\n            self.samples = self.samples[:debug_limit]\n            print(f\"[COCODataset2017] Debug limit: {len(self.samples)} samples\")\n\n        print(f\"Loaded {len(self.samples)} (image, caption) pairs from {captions_json}\")\n\n    def _default_transform(self):\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n        ])\n\n    def __len__(self) -> int:\n        return len(self.samples)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        file_name, tokens = self.samples[idx]\n        img_path = os.path.join(self.images_root, file_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n\n        seq_ids = [bos_id] + self.vocab.numericalize(tokens) + [eos_id]\n        if len(seq_ids) < self.max_len:\n            seq_ids += [pad_id] * (self.max_len - len(seq_ids))\n        else:\n            seq_ids = seq_ids[:self.max_len]\n\n        caption = torch.tensor(seq_ids, dtype=torch.long)\n\n        return {\n            \"image\": image,\n            \"caption\": caption,\n            \"file_name\": file_name,\n        }\n\ndef coco_collate_fn(batch):\n    images = torch.stack([b[\"image\"] for b in batch], dim=0)\n    captions = torch.stack([b[\"caption\"] for b in batch], dim=0)\n    file_names = [b[\"file_name\"] for b in batch]\n    return {\n        \"image\": images,\n        \"caption\": captions,\n        \"file_name\": file_names,\n    }\n\ntrain_dataset = COCODataset2017(\n    images_root=TRAIN_IMAGES_DIR,\n    captions_json=TRAIN_JSON,\n    vocab=vocab,\n    max_len=MAX_LEN,\n    debug_limit=DEBUG_LIMIT,\n)\n\nval_dataset = COCODataset2017(\n    images_root=VAL_IMAGES_DIR,\n    captions_json=VAL_JSON,\n    vocab=vocab,\n    max_len=MAX_LEN,\n    debug_limit=VAL_DEBUG,\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=coco_collate_fn,\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=coco_collate_fn,\n)\n\nprint(\"Train samples:\", len(train_dataset))\nprint(\"Val samples  :\", len(val_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:23:38.890264Z","iopub.execute_input":"2025-12-08T02:23:38.890541Z","iopub.status.idle":"2025-12-08T02:23:42.433627Z","shell.execute_reply.started":"2025-12-08T02:23:38.890523Z","shell.execute_reply":"2025-12-08T02:23:42.432487Z"}},"outputs":[{"name":"stdout","text":"[COCODataset2017] Debug limit: 10000 samples\nLoaded 10000 (image, caption) pairs from /kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json\n[COCODataset2017] Debug limit: 1000 samples\nLoaded 1000 (image, caption) pairs from /kaggle/input/coco-2017-dataset/coco2017/annotations/captions_val2017.json\nTrain samples: 10000\nVal samples  : 1000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# === Cell 2: EncoderCNN (ResNet-based encoder) ===\n\nclass EncoderCNN(nn.Module):\n    \"\"\"\n    CNN encoder for Show, Attend and Tell:\n    - Use a pretrained ResNet (e.g., ResNet-50)\n    - Remove the final pooling + fc\n    - Apply AdaptiveAvgPool2d to get a fixed spatial size\n    - Output shape: [B, enc_image_size, enc_image_size, encoder_dim]\n    \"\"\"\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        # Remove the final fully connected layer & pooling\n        modules = list(resnet.children())[:-2]  # everything until last conv\n        self.cnn = nn.Sequential(*modules)\n\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune(False)\n\n    def forward(self, images):\n        \"\"\"\n        images: [B, 3, 224, 224]\n        returns: [B, enc_image_size, enc_image_size, encoder_dim]\n        \"\"\"\n        out = self.cnn(images)                      # [B, 2048, H, W]\n        out = self.adaptive_pool(out)               # [B, 2048, enc_image_size, enc_image_size]\n        out = out.permute(0, 2, 3, 1)               # [B, enc_image_size, enc_image_size, 2048]\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks.\n        By default we freeze everything for stability; you can unfreeze later.\n        \"\"\"\n        for p in self.cnn.parameters():\n            p.requires_grad = False\n\n        # Unfreeze some layers if fine_tune=True (e.g., last 2 blocks)\n        if fine_tune:\n            for c in list(self.cnn.children())[-2:]:\n                for p in c.parameters():\n                    p.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:23:43.069891Z","iopub.execute_input":"2025-12-08T02:23:43.070184Z","iopub.status.idle":"2025-12-08T02:23:43.076028Z","shell.execute_reply.started":"2025-12-08T02:23:43.070164Z","shell.execute_reply":"2025-12-08T02:23:43.075033Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# === Cell 3: Attention module and DecoderWithAttention ===\n\nclass Attention(nn.Module):\n    \"\"\"\n    Soft 'additive' attention:\n    Given encoder_out (image features) and decoder hidden state,\n    produce attention weights over image locations and a context vector.\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super().__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # encoder features -> att\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # decoder hidden -> att\n        self.full_att = nn.Linear(attention_dim, 1)               # combine and score\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)                          # over pixels\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        decoder_hidden: [B, decoder_dim]\n        returns:\n            attention_weighted_encoding: [B, encoder_dim]\n            alpha: [B, num_pixels]\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)                       # [B, num_pixels, att_dim]\n        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)       # [B, 1, att_dim]\n        att = self.full_att(self.relu(att1 + att2)).squeeze(2)     # [B, num_pixels]\n        alpha = self.softmax(att)                                  # [B, num_pixels]\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [B, enc_dim]\n        return attention_weighted_encoding, alpha\n\n\nclass DecoderWithAttention(nn.Module):\n    \"\"\"\n    LSTM decoder with attention (Show, Attend and Tell style).\n    We use fixed-length captions with BOS/EOS/PAD.\n    \"\"\"\n    def __init__(\n        self,\n        attention_dim,\n        embed_dim,\n        decoder_dim,\n        vocab_size,\n        encoder_dim=2048,\n        dropout=0.5,\n        max_len=30,\n    ):\n        super().__init__()\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # gating scalar\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n\n        self.max_len = max_len\n        self.vocab_size = vocab_size\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)   # [B, encoder_dim]\n        h = self.init_h(mean_encoder_out)            # [B, decoder_dim]\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions):\n        \"\"\"\n        Forward pass during training (teacher forcing).\n        encoder_out: [B, enc_image_size, enc_image_size, encoder_dim]\n        encoded_captions: [B, T] (with BOS, EOS, PAD)\n        Returns:\n            predictions: [B, T-1, vocab_size]\n            alphas: [B, T-1, num_pixels]\n        We predict tokens for positions 1..T-1 (targets are 1..T, i.e. shifted).\n        \"\"\"\n        B = encoder_out.size(0)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(-1)\n        num_pixels = enc_image_size * enc_image_size\n\n        # Flatten spatial dims\n        encoder_out = encoder_out.view(B, -1, encoder_dim)  # [B, num_pixels, encoder_dim]\n\n        # Prepare embeddings (we'll ignore the last token when feeding)\n        embeddings = self.embedding(encoded_captions)       # [B, T, embed_dim]\n        T = encoded_captions.size(1)\n\n        h, c = self.init_hidden_state(encoder_out)\n\n        preds = []\n        alphas = []\n\n        # Decode from t=0..T-2 (We use caption[t] as input, target is caption[t+1])\n        for t in range(T - 1):\n            batch_emb_t = embeddings[:, t, :]                # [B, embed_dim]\n            context, alpha = self.attention(encoder_out, h)  # [B, enc_dim], [B, num_pixels]\n            gate = self.sigmoid(self.f_beta(h))              # gating [B, enc_dim]\n            context = gate * context\n\n            # LSTMCell input is [embed, context]\n            lstm_input = torch.cat([batch_emb_t, context], dim=1)  # [B, embed_dim+enc_dim]\n            h, c = self.decode_step(lstm_input, (h, c))            # both [B, decoder_dim]\n\n            output = self.fc(self.dropout(h))                      # [B, vocab_size]\n            preds.append(output)\n            alphas.append(alpha)\n\n        preds = torch.stack(preds, dim=1)   # [B, T-1, vocab_size]\n        alphas = torch.stack(alphas, dim=1) # [B, T-1, num_pixels]\n\n        return preds, alphas\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:23:48.374090Z","iopub.execute_input":"2025-12-08T02:23:48.374554Z","iopub.status.idle":"2025-12-08T02:23:48.383405Z","shell.execute_reply.started":"2025-12-08T02:23:48.374531Z","shell.execute_reply":"2025-12-08T02:23:48.382515Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# === Cell 4: EncoderCNN, Attention, DecoderWithAttention, SAT wrapper ===\n\nclass EncoderCNN(nn.Module):\n    \"\"\"\n    CNN encoder: ResNet-50, output spatial feature map.\n    \"\"\"\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        modules = list(resnet.children())[:-2]  # remove avgpool & fc\n        self.cnn = nn.Sequential(*modules)\n\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune(False)\n\n    def forward(self, images):\n        \"\"\"\n        images: [B, 3, 224, 224]\n        returns: [B, enc_image_size, enc_image_size, encoder_dim]\n        \"\"\"\n        out = self.cnn(images)                      # [B, 2048, H, W]\n        out = self.adaptive_pool(out)               # [B, 2048, enc_image_size, enc_image_size]\n        out = out.permute(0, 2, 3, 1)               # [B, enc_image_size, enc_image_size, 2048]\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        \"\"\"\n        Unfreeze last conv blocks if fine_tune=True.\n        \"\"\"\n        for p in self.cnn.parameters():\n            p.requires_grad = False\n\n        if fine_tune:\n            for c in list(self.cnn.children())[-2:]:\n                for p in c.parameters():\n                    p.requires_grad = True\n\nclass Attention(nn.Module):\n    \"\"\"\n    Additive attention over spatial image features.\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super().__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att    = nn.Linear(attention_dim, 1)\n        self.relu        = nn.ReLU()\n        self.softmax     = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        decoder_hidden: [B, decoder_dim]\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)             # [B, num_pixels, att_dim]\n        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [B, 1, att_dim]\n        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [B, num_pixels]\n        alpha = self.softmax(att)                        # [B, num_pixels]\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [B, enc_dim]\n        return attention_weighted_encoding, alpha\n\nclass DecoderWithAttention(nn.Module):\n    \"\"\"\n    LSTM decoder with attention (Show, Attend and Tell).\n    \"\"\"\n    def __init__(\n        self,\n        attention_dim,\n        embed_dim,\n        decoder_dim,\n        vocab_size,\n        encoder_dim=2048,\n        dropout=0.5,\n        max_len=30,\n    ):\n        super().__init__()\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout   = nn.Dropout(dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n\n        self.max_len    = max_len\n        self.vocab_size = vocab_size\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        encoder_out: [B, num_pixels, encoder_dim]\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)  # [B, encoder_dim]\n        h = self.init_h(mean_encoder_out)           # [B, decoder_dim]\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions):\n        \"\"\"\n        encoder_out: [B, H, W, encoder_dim]\n        encoded_captions: [B, T]  (with <bos>, ..., <eos>/<pad>)\n        Returns:\n            preds:  [B, T-1, vocab_size]\n            alphas: [B, T-1, num_pixels]\n        \"\"\"\n        B = encoder_out.size(0)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(-1)\n        num_pixels = enc_image_size * enc_image_size\n\n        # Flatten spatial dims\n        encoder_out = encoder_out.view(B, -1, encoder_dim)  # [B, num_pixels, encoder_dim]\n\n        # Embeddings\n        embeddings = self.embedding(encoded_captions)       # [B, T, embed_dim]\n        T = encoded_captions.size(1)\n\n        h, c = self.init_hidden_state(encoder_out)\n\n        preds = []\n        alphas = []\n\n        # Teacher forcing: input caption[t], target caption[t+1]\n        for t in range(T - 1):\n            batch_emb_t = embeddings[:, t, :]                # [B, embed_dim]\n            context, alpha = self.attention(encoder_out, h)  # [B, enc_dim], [B, num_pixels]\n            gate = self.sigmoid(self.f_beta(h))              # [B, enc_dim]\n            context = gate * context\n\n            lstm_input = torch.cat([batch_emb_t, context], dim=1)  # [B, embed_dim + enc_dim]\n            h, c = self.decode_step(lstm_input, (h, c))            # [B, decoder_dim]\n\n            output = self.fc(self.dropout(h))                      # [B, vocab_size]\n            preds.append(output)\n            alphas.append(alpha)\n\n        preds = torch.stack(preds, dim=1)    # [B, T-1, vocab_size]\n        alphas = torch.stack(alphas, dim=1)  # [B, T-1, num_pixels]\n\n        return preds, alphas\n\nclass ShowAttendTell(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        attention_dim=512,\n        embed_dim=512,\n        decoder_dim=512,\n        encoder_dim=2048,\n        dropout=0.5,\n        max_len=30,\n    ):\n        super().__init__()\n        self.encoder = EncoderCNN()\n        self.decoder = DecoderWithAttention(\n            attention_dim=attention_dim,\n            embed_dim=embed_dim,\n            decoder_dim=decoder_dim,\n            vocab_size=vocab_size,\n            encoder_dim=encoder_dim,\n            dropout=dropout,\n            max_len=max_len,\n        )\n\n    def forward(self, images, captions):\n        encoder_out = self.encoder(images)\n        preds, alphas = self.decoder(encoder_out, captions)\n        return preds, alphas\n\nmodel_sat = ShowAttendTell(\n    vocab_size=len(vocab),\n    attention_dim=512,\n    embed_dim=512,\n    decoder_dim=512,\n    encoder_dim=2048,\n    dropout=0.5,\n    max_len=MAX_LEN,\n).to(device)\n\ncriterion_sat = nn.CrossEntropyLoss(ignore_index=pad_id)\noptimizer_sat = torch.optim.AdamW(\n    filter(lambda p: p.requires_grad, model_sat.parameters()),\n    lr=3e-4,\n    weight_decay=1e-4,\n)\n\nprint(\"Trainable params:\",\n      sum(p.numel() for p in model_sat.parameters() if p.requires_grad))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:23:51.506888Z","iopub.execute_input":"2025-12-08T02:23:51.507200Z","iopub.status.idle":"2025-12-08T02:23:52.975439Z","shell.execute_reply.started":"2025-12-08T02:23:51.507180Z","shell.execute_reply":"2025-12-08T02:23:52.974019Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 232MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Trainable params: 22446734\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# === Cell 5: Training loop for SAT (CE only) ===\n\ndef train_one_epoch_sat(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n\n    for batch in tqdm(loader, desc=\"SAT train (1 epoch)\"):\n        images = batch[\"image\"].to(device)\n        captions = batch[\"caption\"].to(device)\n\n        preds, alphas = model(images, captions)      # preds: [B, T-1, V]\n        B, Tm1, V = preds.size()\n        targets = captions[:, 1:]                    # [B, T-1]\n\n        loss = criterion(\n            preds.reshape(B * Tm1, V),\n            targets.reshape(B * Tm1),\n        )\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\nEPOCHS_SAT = 5  # start small; you can increase later\n\nfor e in range(EPOCHS_SAT):\n    avg_loss = train_one_epoch_sat(model_sat, train_loader, optimizer_sat, criterion_sat, device)\n    print(f\"[SAT] Epoch {e+1}/{EPOCHS_SAT} - loss: {avg_loss:.4f}\")\n    torch.save(model_sat.state_dict(), os.path.join(WORK_DIR, f\"sat_epoch{e+1}.pt\"))\n    print(\"Saved checkpoint:\", f\"sat_epoch{e+1}.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:23:57.885428Z","iopub.execute_input":"2025-12-08T02:23:57.885799Z","iopub.status.idle":"2025-12-08T03:15:53.957052Z","shell.execute_reply.started":"2025-12-08T02:23:57.885779Z","shell.execute_reply":"2025-12-08T03:15:53.956055Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"SAT train (1 epoch):   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed694fa9d7849d797f6abf130234c84"}},"metadata":{}},{"name":"stdout","text":"[SAT] Epoch 1/5 - loss: 4.8408\nSaved checkpoint: sat_epoch1.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"SAT train (1 epoch):   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa03fc6dc6aa482cb5516cffaa009503"}},"metadata":{}},{"name":"stdout","text":"[SAT] Epoch 2/5 - loss: 3.8074\nSaved checkpoint: sat_epoch2.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"SAT train (1 epoch):   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa38c706751d4015a3e01def329dac8d"}},"metadata":{}},{"name":"stdout","text":"[SAT] Epoch 3/5 - loss: 3.4414\nSaved checkpoint: sat_epoch3.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"SAT train (1 epoch):   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30eaa6afff9f49ed84bbfa8a14b9cb44"}},"metadata":{}},{"name":"stdout","text":"[SAT] Epoch 4/5 - loss: 3.1788\nSaved checkpoint: sat_epoch4.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"SAT train (1 epoch):   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae119ea037024e94b38ac30016cf49ff"}},"metadata":{}},{"name":"stdout","text":"[SAT] Epoch 5/5 - loss: 2.9697\nSaved checkpoint: sat_epoch5.pt\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# === Cell 6: Greedy decoding for qualitative check ===\n\ndef ids_to_words(ids, vocab, pad_id, eos_id, drop_bos=True):\n    words = []\n    for idx in ids:\n        if idx == pad_id:\n            break\n        tok = vocab.itos[idx]\n        if drop_bos and tok == \"<bos>\":\n            continue\n        if tok == \"<eos>\":\n            break\n        words.append(tok)\n    return words\n\n@torch.no_grad()\ndef sat_greedy_decode(model, image, max_len=30):\n    \"\"\"\n    Greedy decoding for a single image tensor [3, 224, 224].\n    Returns list of token ids (includes <bos>, ...).\n    \"\"\"\n    model.eval()\n    image = image.unsqueeze(0).to(device)    # [1, 3, 224, 224]\n\n    encoder_out = model.encoder(image)       # [1, H, W, enc_dim]\n    B, H, W, enc_dim = encoder_out.shape\n    num_pixels = H * W\n    encoder_out = encoder_out.view(1, num_pixels, enc_dim)   # [1, num_pixels, enc_dim]\n\n    h, c = model.decoder.init_hidden_state(encoder_out)\n\n    seq = [bos_id]\n    prev_word = torch.tensor([bos_id], device=device, dtype=torch.long)\n    prev_emb = model.decoder.embedding(prev_word)            # [1, embed_dim]\n\n    for _ in range(max_len - 1):\n        context, alpha = model.decoder.attention(encoder_out, h)\n        gate = model.decoder.sigmoid(model.decoder.f_beta(h))\n        context = gate * context\n\n        lstm_input = torch.cat([prev_emb, context], dim=1)\n        h, c = model.decoder.decode_step(lstm_input, (h, c))\n\n        output = model.decoder.fc(model.decoder.dropout(h))  # [1, vocab_size]\n        _, next_word = output.max(dim=1)\n        next_id = next_word.item()\n        seq.append(next_id)\n\n        if next_id == eos_id:\n            break\n\n        prev_emb = model.decoder.embedding(next_word)\n\n    return seq\n\n# --- Test on a small batch from val_loader ---\nbatch = next(iter(val_loader))\nimages = batch[\"image\"]\ncaptions = batch[\"caption\"]\nfile_names = batch[\"file_name\"]\n\nfor i in range(10):\n    img = images[i]\n    gt_ids = captions[i].tolist()\n    pred_ids = sat_greedy_decode(model_sat, img, max_len=MAX_LEN)\n\n    gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n    pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n    print(\"\\nFile:\", file_names[i])\n    print(\"PRED:\", \" \".join(pred_words) if pred_words else \"[EMPTY]\")\n    print(\"GT  :\", \" \".join(gt_words))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T00:55:26.839294Z","iopub.execute_input":"2025-12-08T00:55:26.839582Z","iopub.status.idle":"2025-12-08T00:55:27.756864Z","shell.execute_reply.started":"2025-12-08T00:55:26.839565Z","shell.execute_reply":"2025-12-08T00:55:27.755872Z"}},"outputs":[{"name":"stdout","text":"\nFile: 000000179765.jpg\nPRED: a motorcycle parked in a parking lot\nGT  : a black honda motorcycle parked in front of a garage\n\nFile: 000000179765.jpg\nPRED: a motorcycle parked in a parking lot\nGT  : a honda motorcycle parked in a grass driveway\n\nFile: 000000190236.jpg\nPRED: a kitchen with a stove and a computer\nGT  : an office cubicle with four different types of computers\n\nFile: 000000331352.jpg\nPRED: a bathroom with a toilet and a sink\nGT  : a small closed toilet in a cramped space\n\nFile: 000000517069.jpg\nPRED: a man sitting on a street with a city street\nGT  : two women waiting at a bench next to a street\n\nFile: 000000179765.jpg\nPRED: a motorcycle parked in a parking lot\nGT  : a black honda motorcycle with a dark burgundy seat\n\nFile: 000000331352.jpg\nPRED: a bathroom with a toilet and a sink\nGT  : a tan toilet and sink combination in a small room\n\nFile: 000000190236.jpg\nPRED: a kitchen with a stove and a computer\nGT  : the home office space seems to be very cluttered\n\nFile: 000000182417.jpg\nPRED: a bowl of a plate with a plate with a plate and a table\nGT  : a beautiful dessert waiting to be shared by two people\n\nFile: 000000517069.jpg\nPRED: a man sitting on a street with a city street\nGT  : a woman sitting on a bench and a woman standing waiting for the bus\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# === Cell 7: Beam search decoding for Show, Attend and Tell ===\n\nimport math\n\n@torch.no_grad()\ndef sat_beam_search_single(\n    model,\n    image,          # [3, 224, 224]\n    bos_id,\n    eos_id,\n    pad_id,\n    beam_size=3,\n    max_len=30,\n):\n    \"\"\"\n    Beam search for a single image.\n    Returns: list of token ids (including <bos>, ..., <eos>).\n    \"\"\"\n    model.eval()\n\n    # Encode image\n    img = image.unsqueeze(0).to(device)           # [1, 3, H, W]\n    encoder_out = model.encoder(img)              # [1, H, W, enc_dim]\n    B, H, W, enc_dim = encoder_out.shape\n    num_pixels = H * W\n    encoder_out = encoder_out.view(1, num_pixels, enc_dim)  # [1, num_pixels, enc_dim]\n\n    # Init hidden state\n    h, c = model.decoder.init_hidden_state(encoder_out)     # [1, dec_dim]\n\n    # Beam: list of (log_prob, seq, h, c)\n    # seq is a list of token ids\n    start_seq = [bos_id]\n    beam = [(0.0, start_seq, h, c)]   # log_prob = 0\n\n    completed = []\n\n    for _ in range(max_len - 1):\n        new_beam = []\n\n        for log_p, seq, h_prev, c_prev in beam:\n            # If already ended, keep in completed\n            if seq[-1] == eos_id:\n                completed.append((log_p, seq))\n                continue\n\n            prev_word_id = seq[-1]\n            prev_word = torch.tensor([prev_word_id], device=device, dtype=torch.long)\n            prev_emb = model.decoder.embedding(prev_word)             # [1, embed_dim]\n\n            # Attention\n            context, alpha = model.decoder.attention(encoder_out, h_prev)\n            gate = model.decoder.sigmoid(model.decoder.f_beta(h_prev))\n            context = gate * context\n\n            lstm_input = torch.cat([prev_emb, context], dim=1)        # [1, emb+enc]\n            h_new, c_new = model.decoder.decode_step(lstm_input, (h_prev, c_prev))\n\n            output = model.decoder.fc(model.decoder.dropout(h_new))   # [1, vocab_size]\n            log_probs = torch.log_softmax(output, dim=-1).squeeze(0)  # [vocab_size]\n\n            # Top-k expansions\n            topk_logp, topk_ids = torch.topk(log_probs, beam_size)\n\n            for lp, idx in zip(topk_logp.tolist(), topk_ids.tolist()):\n                new_seq = seq + [idx]\n                new_log_p = log_p + lp\n                new_beam.append((new_log_p, new_seq, h_new, c_new))\n\n        if not new_beam:\n            break\n\n        # Keep top beam_size\n        new_beam.sort(key=lambda x: x[0], reverse=True)\n        beam = new_beam[:beam_size]\n\n        # If we already have enough completed sequences and all beams ended, break\n        all_ended = all(seq[-1] == eos_id for _, seq, _, _ in beam)\n        if all_ended:\n            break\n\n    # If we found completed sequences, pick the best\n    if completed:\n        completed.sort(key=lambda x: x[0], reverse=True)\n        best_logp, best_seq = completed[0]\n    else:\n        # Otherwise, use the best in the current beam\n        beam.sort(key=lambda x: x[0], reverse=True)\n        best_logp, best_seq, _, _ = beam[0]\n\n    return best_seq\n\n@torch.no_grad()\ndef sat_beam_search_batch(\n    model,\n    images,     # [B, 3, H, W]\n    bos_id,\n    eos_id,\n    pad_id,\n    beam_size=3,\n    max_len=30,\n):\n    \"\"\"\n    Run beam search for each image in a batch independently.\n    Returns: list of lists of ids.\n    \"\"\"\n    preds = []\n    B = images.size(0)\n    for i in range(B):\n        seq = sat_beam_search_single(\n            model,\n            images[i],\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=max_len,\n        )\n        preds.append(seq)\n    return preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T00:46:24.079977Z","iopub.execute_input":"2025-12-08T00:46:24.080196Z","iopub.status.idle":"2025-12-08T00:46:24.089934Z","shell.execute_reply.started":"2025-12-08T00:46:24.080161Z","shell.execute_reply":"2025-12-08T00:46:24.089084Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Quick sanity: compare greedy vs beam for a few val images\nbatch = next(iter(val_loader))\nimages = batch[\"image\"]\ncaptions = batch[\"caption\"]\nfile_names = batch[\"file_name\"]\n\nfor i in range(10):\n    img = images[i]\n    gt_ids = captions[i].tolist()\n\n    greedy_ids = sat_greedy_decode(model_sat, img, max_len=MAX_LEN)\n    beam_ids   = sat_beam_search_single(model_sat, img, bos_id, eos_id, pad_id, beam_size=3, max_len=MAX_LEN)\n\n    greedy_words = ids_to_words(greedy_ids, vocab, pad_id, eos_id)\n    beam_words   = ids_to_words(beam_ids, vocab, pad_id, eos_id)\n    gt_words     = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n\n    print(\"\\nFile:\", file_names[i])\n    print(\"GREEDY:\", \" \".join(greedy_words))\n    print(\"BEAM  :\", \" \".join(beam_words))\n    print(\"GT    :\", \" \".join(gt_words))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T00:55:27.757330Z","iopub.execute_input":"2025-12-08T00:55:27.757647Z","iopub.status.idle":"2025-12-08T00:55:30.412194Z","shell.execute_reply.started":"2025-12-08T00:55:27.757629Z","shell.execute_reply":"2025-12-08T00:55:30.411546Z"}},"outputs":[{"name":"stdout","text":"\nFile: 000000179765.jpg\nGREEDY: a motorcycle parked in a parking lot\nBEAM  : a motorcycle parked next to a motorcycle\nGT    : a black honda motorcycle parked in front of a garage\n\nFile: 000000179765.jpg\nGREEDY: a motorcycle parked in a parking lot\nBEAM  : a motorcycle parked next to a motorcycle\nGT    : a honda motorcycle parked in a grass driveway\n\nFile: 000000190236.jpg\nGREEDY: a kitchen with a stove and a computer\nBEAM  : a kitchen with a stove and a computer\nGT    : an office cubicle with four different types of computers\n\nFile: 000000331352.jpg\nGREEDY: a bathroom with a toilet and a sink\nBEAM  : a bathroom with a toilet and a sink\nGT    : a small closed toilet in a cramped space\n\nFile: 000000517069.jpg\nGREEDY: a man sitting on a street with a city street\nBEAM  : a group of people sitting on the street\nGT    : two women waiting at a bench next to a street\n\nFile: 000000179765.jpg\nGREEDY: a motorcycle parked in a parking lot\nBEAM  : a motorcycle parked next to a motorcycle\nGT    : a black honda motorcycle with a dark burgundy seat\n\nFile: 000000331352.jpg\nGREEDY: a bathroom with a toilet and a sink\nBEAM  : a bathroom with a toilet and a sink\nGT    : a tan toilet and sink combination in a small room\n\nFile: 000000190236.jpg\nGREEDY: a kitchen with a stove and a computer\nBEAM  : a kitchen with a stove and a computer\nGT    : the home office space seems to be very cluttered\n\nFile: 000000182417.jpg\nGREEDY: a bowl of a plate with a plate with a plate and a table\nBEAM  : a picture of a plate of a plate with a plate\nGT    : a beautiful dessert waiting to be shared by two people\n\nFile: 000000517069.jpg\nGREEDY: a man sitting on a street with a city street\nBEAM  : a group of people sitting on the street\nGT    : a woman sitting on a bench and a woman standing waiting for the bus\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# === Cell 8: BLEU-4 evaluation (beam search) ===\n\n!pip install nltk -q\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nnltk.download('punkt', quiet=True)\n\ndef evaluate_bleu_sat_beam(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=50,   # for speed; set to len(loader) for full\n):\n    model.eval()\n    smoothie = SmoothingFunction().method4\n    scores = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"BLEU eval (SAT beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        captions = batch[\"caption\"]\n\n        pred_ids_batch = sat_beam_search_batch(\n            model,\n            images,\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=MAX_LEN,\n        )\n\n        B = captions.size(0)\n        for i in range(B):\n            gt_ids   = captions[i].tolist()\n            pred_ids = pred_ids_batch[i]\n\n            gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n            pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n            if len(pred_words) == 0 or len(gt_words) == 0:\n                continue\n\n            score = sentence_bleu(\n                [gt_words],\n                pred_words,\n                smoothing_function=smoothie,\n                weights=(0.25, 0.25, 0.25, 0.25),\n            )\n            scores.append(score)\n\n    if not scores:\n        return 0.0\n    return float(np.mean(scores))\n\nbleu_beam = evaluate_bleu_sat_beam(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=300,   # ~30 batches for a quick estimate\n)\n\nprint(\"Approx BLEU-4 (SAT + beam=3):\", bleu_beam)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T00:46:25.015138Z","iopub.execute_input":"2025-12-08T00:46:25.015347Z","iopub.status.idle":"2025-12-08T00:49:39.271605Z","shell.execute_reply.started":"2025-12-08T00:46:25.015329Z","shell.execute_reply":"2025-12-08T00:49:39.270701Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"BLEU eval (SAT beam):   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a555ee3d34a47e29f23aa3ee0d167ee"}},"metadata":{}},{"name":"stdout","text":"Approx BLEU-4 (SAT + beam=3): 0.07269644203025069\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# === Cell 9: CLIPScore-style evaluation for SAT (beam captions) ===\n\n!pip install open_clip_torch -q\nimport open_clip\nfrom PIL import Image\n\n# Load CLIP model + preprocess once\nclip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n    \"ViT-B-32\",\n    pretrained=\"openai\",\n)\nclip_model = clip_model.to(device).eval()\nclip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n\n@torch.no_grad()\ndef compute_clipscore_sat_beam(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=10,  # small for speed; increase if you want\n):\n    sims = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CLIPScore eval (SAT beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]   # [B, 3, H, W]\n        file_names = batch[\"file_name\"]\n\n        # Decode captions with beam search\n        pred_ids_batch = sat_beam_search_batch(\n            model,\n            images,\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=MAX_LEN,\n        )\n        pred_texts = []\n        for seq in pred_ids_batch:\n            pred_texts.append(\" \".join(ids_to_words(seq, vocab, pad_id, eos_id)))\n\n        # For each image–caption pair, compute CLIP similarity\n        for img_tensor, cap in zip(images, pred_texts):\n            # Convert tensor -> PIL, apply CLIP preprocess\n            pil = transforms.ToPILImage()(img_tensor)\n            image_input = clip_preprocess(pil).unsqueeze(0).to(device)\n            text_tokens = clip_tokenizer([cap]).to(device)\n\n            img_feat = clip_model.encode_image(image_input)\n            txt_feat = clip_model.encode_text(text_tokens)\n\n            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n            txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n            sim = (img_feat * txt_feat).sum(dim=-1).item()\n            sims.append(sim)\n\n    sims = np.array(sims)\n    stats = {\n        \"mean\": float(sims.mean()),\n        \"median\": float(np.median(sims)),\n        \"std\": float(sims.std()),\n        \"low_clip_rate\": float((sims < 0.2).mean()),\n    }\n    return stats\n\nclip_stats_sat = compute_clipscore_sat_beam(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    beam_size=3,\n    max_batches=10,  # just a sample for speed\n)\nprint(\"CLIPScore stats (SAT + beam=3):\", clip_stats_sat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T00:49:39.272203Z","iopub.execute_input":"2025-12-08T00:49:39.272502Z","iopub.status.idle":"2025-12-08T00:51:24.251891Z","shell.execute_reply.started":"2025-12-08T00:49:39.272485Z","shell.execute_reply":"2025-12-08T00:51:24.250658Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce0cb70cd3948668f706c240f97704b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"CLIPScore eval (SAT beam):   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"101a683aedaa49c98e08653d3a3df368"}},"metadata":{}},{"name":"stdout","text":"CLIPScore stats (SAT + beam=3): {'mean': 0.22763739149086176, 'median': 0.22704041749238968, 'std': 0.023720598154584698, 'low_clip_rate': 0.096875}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# === Cell 10: CHAIR hallucination metrics for SAT (beam captions) ===\n\n# 1) Load instance annotations (COCO 2017 val)\nINST_VAL_JSON = os.path.join(ANN_DIR, \"instances_val2017.json\")\nwith open(INST_VAL_JSON, \"r\") as f:\n    inst_val = json.load(f)\n\n# Map image_id -> file_name and category_id -> name\nimgid_to_file = {img[\"id\"]: img[\"file_name\"] for img in inst_val[\"images\"]}\ncatid_to_name = {c[\"id\"]: c[\"name\"].lower() for c in inst_val[\"categories\"]}\n\n# Build mapping from file_name -> set of ground-truth object classes\nfrom collections import defaultdict\nfile_to_objects = defaultdict(set)\nfor ann in inst_val[\"annotations\"]:\n    img_id = ann[\"image_id\"]\n    cat_id = ann[\"category_id\"]\n    file_name = imgid_to_file[img_id]\n    cat_name = catid_to_name[cat_id]\n    file_to_objects[file_name].add(cat_name)\n\n# Category vocab for matching words in captions\ndef simple_tokenize(text: str):\n    return re.findall(r\"[a-z]+\", text.lower())\n\ncategory_vocab = []\nfor cat_name in sorted(set(catid_to_name.values())):\n    tokens = simple_tokenize(cat_name)\n    category_vocab.append((cat_name, tokens))\n\ndef find_mentioned_objects(caption_text: str, category_vocab):\n    tokens = simple_tokenize(caption_text)\n    mentioned = set()\n    for cat_name, cat_tokens in category_vocab:\n        L = len(cat_tokens)\n        if L == 1:\n            if cat_tokens[0] in tokens:\n                mentioned.add(cat_name)\n        else:\n            # multi-word categories like \"traffic light\"\n            for i in range(len(tokens) - L + 1):\n                if tokens[i:i+L] == cat_tokens:\n                    mentioned.add(cat_name)\n                    break\n    return mentioned\n\n@torch.no_grad()\ndef compute_chair_sat_beam(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    beam_size=3,\n    max_batches=30,\n):\n    model.eval()\n    all_caps = 0\n    caps_with_hallucination = 0\n    total_mentions = 0\n    hallucinated_mentions = 0\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CHAIR eval (SAT beam)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        file_names = batch[\"file_name\"]\n\n        pred_ids_batch = sat_beam_search_batch(\n            model,\n            images,\n            bos_id,\n            eos_id,\n            pad_id,\n            beam_size=beam_size,\n            max_len=MAX_LEN,\n        )\n        pred_texts = [\n            \" \".join(ids_to_words(seq, vocab, pad_id, eos_id))\n            for seq in pred_ids_batch\n        ]\n\n        for fn, cap in zip(file_names, pred_texts):\n            all_caps += 1\n            gt_objects = file_to_objects.get(fn, set())\n            mentioned = find_mentioned_objects(cap, category_vocab)\n            if not mentioned:\n                continue\n\n            total_mentions += len(mentioned)\n            hallucinated = mentioned - gt_objects\n            if hallucinated:\n                caps_with_hallucination += 1\n                hallucinated_mentions += len(hallucinated)\n\n    chair_s = caps_with_hallucination / all_caps if all_caps else 0.0\n    chair_i = hallucinated_mentions / total_mentions if total_mentions else 0.0\n    return {\n        \"CHAIRs\": chair_s,\n        \"CHAIRi\": chair_i,\n        \"total_captions\": all_caps,\n        \"total_mentions\": total_mentions,\n        \"hallucinated_mentions\": hallucinated_mentions,\n    }\n\nchair_stats_sat = compute_chair_sat_beam(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    beam_size=3,\n    max_batches=32,\n)\nprint(\"CHAIR stats (SAT + beam=3):\", chair_stats_sat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T02:22:09.952982Z","iopub.execute_input":"2025-12-08T02:22:09.953120Z","iopub.status.idle":"2025-12-08T02:22:10.129852Z","shell.execute_reply.started":"2025-12-08T02:22:09.953105Z","shell.execute_reply":"2025-12-08T02:22:10.128834Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Cell 10: CHAIR hallucination metrics for SAT (beam captions) ===\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1) Load instance annotations (COCO 2017 val)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m INST_VAL_JSON = \u001b[43mos\u001b[49m.path.join(ANN_DIR, \u001b[33m\"\u001b[39m\u001b[33minstances_val2017.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(INST_VAL_JSON, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     inst_val = json.load(f)\n","\u001b[31mNameError\u001b[39m: name 'os' is not defined"],"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === NEW CELL: BLEU-4 evaluation (SAT + greedy) ===\n\ndef evaluate_bleu_sat_greedy(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=50,   # for speed; set to len(loader) for full\n):\n    model.eval()\n    smoothie = SmoothingFunction().method4\n    scores = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"BLEU eval (SAT greedy)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        captions = batch[\"caption\"]\n\n        B = images.size(0)\n        for i in range(B):\n            img = images[i]\n            gt_ids = captions[i].tolist()\n\n            # Greedy decoding for this image\n            pred_ids = sat_greedy_decode(model, img, max_len=MAX_LEN)\n\n            gt_words   = ids_to_words(gt_ids, vocab, pad_id, eos_id)\n            pred_words = ids_to_words(pred_ids, vocab, pad_id, eos_id)\n\n            if len(pred_words) == 0 or len(gt_words) == 0:\n                continue\n\n            score = sentence_bleu(\n                [gt_words],\n                pred_words,\n                smoothing_function=smoothie,\n                weights=(0.25, 0.25, 0.25, 0.25),\n            )\n            scores.append(score)\n\n    if not scores:\n        return 0.0\n    return float(np.mean(scores))\n\nbleu_greedy = evaluate_bleu_sat_greedy(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=300,  # adjust as you like\n)\n\nprint(\"Approx BLEU-4 (SAT + greedy):\", bleu_greedy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T01:09:29.829054Z","iopub.execute_input":"2025-12-08T01:09:29.829343Z","iopub.status.idle":"2025-12-08T01:11:00.378902Z","shell.execute_reply.started":"2025-12-08T01:09:29.829324Z","shell.execute_reply":"2025-12-08T01:11:00.378059Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"BLEU eval (SAT greedy):   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13fcadd3fb0242379b99c147838b3845"}},"metadata":{}},{"name":"stdout","text":"Approx BLEU-4 (SAT + greedy): 0.07183989813031917\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# === NEW CELL: CLIPScore evaluation (SAT + greedy) ===\n\n@torch.no_grad()\ndef compute_clipscore_sat_greedy(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=10,  # small for speed; increase if needed\n):\n    model.eval()\n    sims = []\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CLIPScore eval (SAT greedy)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]   # [B, 3, H, W]\n\n        B = images.size(0)\n        for i in range(B):\n            img_tensor = images[i]\n\n            # Greedy caption\n            pred_ids = sat_greedy_decode(model, img_tensor, max_len=MAX_LEN)\n            caption  = \" \".join(ids_to_words(pred_ids, vocab, pad_id, eos_id))\n\n            # Convert tensor -> PIL, apply CLIP preprocess\n            pil = transforms.ToPILImage()(img_tensor)\n            image_input = clip_preprocess(pil).unsqueeze(0).to(device)\n            text_tokens = clip_tokenizer([caption]).to(device)\n\n            img_feat = clip_model.encode_image(image_input)\n            txt_feat = clip_model.encode_text(text_tokens)\n\n            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n            txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n            sim = (img_feat * txt_feat).sum(dim=-1).item()\n            sims.append(sim)\n\n    sims = np.array(sims)\n    stats = {\n        \"mean\": float(sims.mean()),\n        \"median\": float(np.median(sims)),\n        \"std\": float(sims.std()),\n        \"low_clip_rate\": float((sims < 0.2).mean()),\n    }\n    return stats\n\nclip_stats_sat_greedy = compute_clipscore_sat_greedy(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    max_batches=32,\n)\nprint(\"CLIPScore stats (SAT + greedy):\", clip_stats_sat_greedy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T01:13:27.712087Z","iopub.execute_input":"2025-12-08T01:13:27.712518Z","iopub.status.idle":"2025-12-08T01:15:58.740322Z","shell.execute_reply.started":"2025-12-08T01:13:27.712499Z","shell.execute_reply":"2025-12-08T01:15:58.739706Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"CLIPScore eval (SAT greedy):   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610a7d9649464e74a8782518c6156b56"}},"metadata":{}},{"name":"stdout","text":"CLIPScore stats (SAT + greedy): {'mean': 0.22726542752981185, 'median': 0.22602428495883942, 'std': 0.02360034516301436, 'low_clip_rate': 0.122}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# === NEW CELL: CHAIR hallucination metrics (SAT + greedy) ===\n\n@torch.no_grad()\ndef compute_chair_sat_greedy(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    max_batches=30,\n):\n    model.eval()\n    all_caps = 0\n    caps_with_hallucination = 0\n    total_mentions = 0\n    hallucinated_mentions = 0\n\n    for b_idx, batch in enumerate(tqdm(loader, desc=\"CHAIR eval (SAT greedy)\")):\n        if b_idx >= max_batches:\n            break\n\n        images = batch[\"image\"]\n        file_names = batch[\"file_name\"]\n\n        B = images.size(0)\n        for i in range(B):\n            img = images[i]\n            fn  = file_names[i]\n\n            pred_ids = sat_greedy_decode(model, img, max_len=MAX_LEN)\n            cap_text = \" \".join(ids_to_words(pred_ids, vocab, pad_id, eos_id))\n\n            all_caps += 1\n            gt_objects = file_to_objects.get(fn, set())\n            mentioned = find_mentioned_objects(cap_text, category_vocab)\n            if not mentioned:\n                continue\n\n            total_mentions += len(mentioned)\n            hallucinated = mentioned - gt_objects\n            if hallucinated:\n                caps_with_hallucination += 1\n                hallucinated_mentions += len(hallucinated)\n\n    chair_s = caps_with_hallucination / all_caps if all_caps else 0.0\n    chair_i = hallucinated_mentions / total_mentions if total_mentions else 0.0\n    return {\n        \"CHAIRs\": chair_s,\n        \"CHAIRi\": chair_i,\n        \"total_captions\": all_caps,\n        \"total_mentions\": total_mentions,\n        \"hallucinated_mentions\": hallucinated_mentions,\n    }\n\nchair_stats_sat_greedy = compute_chair_sat_greedy(\n    model_sat,\n    val_loader,\n    vocab,\n    pad_id,\n    eos_id,\n    device,\n    file_to_objects,\n    category_vocab,\n    max_batches=32,\n)\nprint(\"CHAIR stats (SAT + greedy):\", chair_stats_sat_greedy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T01:15:58.740816Z","iopub.execute_input":"2025-12-08T01:15:58.740982Z","iopub.status.idle":"2025-12-08T01:17:18.825020Z","shell.execute_reply.started":"2025-12-08T01:15:58.740967Z","shell.execute_reply":"2025-12-08T01:17:18.824218Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"CHAIR eval (SAT greedy):   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6958b8b862fe416c8c983a306d55c5a8"}},"metadata":{}},{"name":"stdout","text":"CHAIR stats (SAT + greedy): {'CHAIRs': 0.257, 'CHAIRi': 0.3397727272727273, 'total_captions': 1000, 'total_mentions': 880, 'hallucinated_mentions': 299}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# === Cell: Visualize SAT predictions (greedy + beam) with images ===\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# If you normalized images with ImageNet stats, define unnormalize:\nIMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\nIMAGENET_STD  = np.array([0.229, 0.224, 0.225])\n\ndef unnormalize(img_tensor):\n    \"\"\"\n    img_tensor: [3, H, W] tensor in normalized space.\n    Returns HxWx3 numpy array in [0,1].\n    \"\"\"\n    img = img_tensor.cpu().numpy()\n    img = (img * IMAGENET_STD[:, None, None]) + IMAGENET_MEAN[:, None, None]\n    img = np.clip(img, 0.0, 1.0)\n    img = np.transpose(img, (1, 2, 0))  # CHW -> HWC\n    return img\n\ndef decode_ids_to_text(ids, vocab, pad_id, eos_id):\n    return \" \".join(ids_to_words(ids, vocab, pad_id, eos_id))\n\n@torch.no_grad()\ndef show_sat_examples(\n    model,\n    loader,\n    vocab,\n    pad_id,\n    eos_id,\n    bos_id,\n    device,\n    num_examples=5,\n    use_beam=True,\n    beam_size=3,\n    max_len=30,\n):\n    \"\"\"\n    Show a few validation images with:\n      - ground truth caption\n      - SAT greedy prediction\n      - SAT beam prediction (optional)\n    \"\"\"\n    model.eval()\n    \n    batch = next(iter(loader))\n    images = batch[\"image\"]\n    captions = batch[\"caption\"]\n    file_names = batch.get(\"file_name\", None)\n\n    n = min(num_examples, images.size(0))\n\n    for i in range(n):\n        img_tensor = images[i]\n        gt_ids = captions[i].tolist()\n\n        # Greedy prediction\n        pred_ids_greedy = sat_greedy_decode(\n            model,\n            img_tensor,\n            max_len=max_len\n        )\n\n        # Beam prediction (optional)\n        if use_beam:\n            pred_ids_beam = sat_beam_search_single(\n                model,\n                img_tensor,\n                bos_id,\n                eos_id,\n                pad_id,\n                beam_size=beam_size,\n                max_len=max_len,\n            )\n        else:\n            pred_ids_beam = None\n\n        gt_text       = decode_ids_to_text(gt_ids, vocab, pad_id, eos_id)\n        pred_text_gr  = decode_ids_to_text(pred_ids_greedy, vocab, pad_id, eos_id)\n        pred_text_beam = decode_ids_to_text(pred_ids_beam, vocab, pad_id, eos_id) if pred_ids_beam else \"[disabled]\"\n\n        # Plot image\n        plt.figure(figsize=(6, 6))\n        plt.imshow(unnormalize(img_tensor))\n        plt.axis(\"off\")\n\n        title = f\"Example {i+1}\"\n        if file_names is not None:\n            title += f\"  ({file_names[i]})\"\n        plt.title(title, fontsize=12)\n\n        # Print captions under the image\n        print(\"=\" * 80)\n        print(title)\n        print(\"GT      :\", gt_text)\n        print(\"Greedy  :\", pred_text_gr if pred_text_gr else \"[EMPTY]\")\n        if use_beam:\n            print(f\"Beam({beam_size}):\", pred_text_beam if pred_text_beam else \"[EMPTY]\")\n\n        plt.show()\n\n\n# --- Call it (SAT model) ---\nshow_sat_examples(\n    model=model_sat,\n    loader=val_loader,\n    vocab=vocab,\n    pad_id=pad_id,\n    eos_id=eos_id,\n    bos_id=bos_id,\n    device=device,\n    num_examples=10,\n    use_beam=True,   # set False if you only want greedy\n    beam_size=3,\n    max_len=MAX_LEN,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T03:31:27.041677Z","iopub.execute_input":"2025-12-08T03:31:27.042020Z","iopub.status.idle":"2025-12-08T03:31:29.337543Z","shell.execute_reply.started":"2025-12-08T03:31:27.042000Z","shell.execute_reply":"2025-12-08T03:31:29.336351Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    100\u001b[39m         plt.show()\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# --- Call it (SAT model) ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[43mshow_sat_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_sat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_beam\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# set False if you only want greedy\u001b[39;49;00m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mshow_sat_examples\u001b[39m\u001b[34m(model, loader, vocab, pad_id, eos_id, bos_id, device, num_examples, use_beam, beam_size, max_len)\u001b[39m\n\u001b[32m     55\u001b[39m gt_ids = captions[i].tolist()\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Greedy prediction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m pred_ids_greedy = \u001b[43msat_greedy_decode\u001b[49m(\n\u001b[32m     59\u001b[39m     model,\n\u001b[32m     60\u001b[39m     img_tensor,\n\u001b[32m     61\u001b[39m     max_len=max_len\n\u001b[32m     62\u001b[39m )\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Beam prediction (optional)\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_beam:\n","\u001b[31mNameError\u001b[39m: name 'sat_greedy_decode' is not defined"],"ename":"NameError","evalue":"name 'sat_greedy_decode' is not defined","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}